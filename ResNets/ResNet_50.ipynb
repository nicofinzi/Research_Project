{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZhn90Gxi8Fn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14771c2c-aa5f-466b-c549-c69a27e96c2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install medmnist"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jNoBxg-cYNum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import all necessary libraries and functions\n",
        "\n",
        "import os, math, random\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "from medmnist import INFO\n",
        "from medmnist.dataset import PathMNIST\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "o9O235mkYNwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configuration\n",
        "class CFG:\n",
        "    # path where PathMNIST data will be stored\n",
        "    data_root: str = \"/content/drive/MyDrive/Colab Notebooks/DISSERTATION/CNNs/ResNet-50\"\n",
        "    download: bool = True\n",
        "\n",
        "    # training hyperparameters\n",
        "    epochs: int = 10                # no. of training epochs\n",
        "    batch_size: int = 128           # batch size\n",
        "    lr: float = 3e-4                # learning rate\n",
        "    weight_decay: float = 1e-4      # weight decay schedule\n",
        "    warmup_epochs: int = 3          # no. of warmup epochs\n",
        "    label_smoothing: float = 0.0    # label smoothing for classification loss\n",
        "    amp: bool = True                # mixed precision\n",
        "\n",
        "    # model\n",
        "    use_pretrained: bool = True     # True --> ImageNet pretrained weights\n",
        "    dropout: float = 0.0\n",
        "\n",
        "    # system parameters\n",
        "    workers: int = 4                # no. of dataloader workers\n",
        "    seed: int = 42                  # random seed for reproducibility\n",
        "    gpu: int = 0                    # GPU to use\n",
        "\n",
        "    # path to save model checkpoints\n",
        "    ckpt_path: str = \"/content/drive/MyDrive/Colab Notebooks/DISSERTATION/CNNs/ResNet-18/resnet18_pathmnist_best.pth\"\n",
        "\n",
        "\n",
        "CFG = CFG()\n",
        "\n",
        "# reproducibility helpers\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# apply seed\n",
        "set_seed(CFG.seed)\n",
        "\n",
        "# device setup\n",
        "device = torch.device(f\"cuda:{CFG.gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBtiCHt3ZRAa",
        "outputId": "0c3fb639-fa21-4eb6-aa98-971109ddd4c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transforms\n",
        "NUM_CLASSES = 9           # no. of PathMNIST tissue classes\n",
        "info = INFO[\"pathmnist\"]\n",
        "\n",
        "# MedMNIST default normalisation values\n",
        "pm_mean = (0.5, 0.5, 0.5)\n",
        "pm_std = (0.5, 0.5, 0.5)\n",
        "\n",
        "# training data transformations\n",
        "train_tfms = transforms.Compose([\n",
        "    # random crop + resize image to 224x224, scale crop between 80%-100% of original image\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0), interpolation=InterpolationMode.BILINEAR),\n",
        "    # random horizontal flip\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # convert PIL image to PyTorch tensor\n",
        "    transforms.ToTensor(),\n",
        "    # normalise channels using MedMNIST default mean and std\n",
        "    transforms.Normalize(mean=pm_mean, std=pm_std),\n",
        "])\n",
        "# validation/testing data transformations\n",
        "eval_tfms = transforms.Compose([\n",
        "    transforms.Resize(256, interpolation=InterpolationMode.BILINEAR),\n",
        "    # crop central region 224x224\n",
        "    transforms.CenterCrop(224),\n",
        "    # convert PIL image to PyTorch tensor\n",
        "    transforms.ToTensor(),\n",
        "    # normalise channels using MedMNIST default mean and std\n",
        "    transforms.Normalize(mean=pm_mean, std=pm_std),\n",
        "])\n",
        "\n",
        "# load PathMNIST training, validation and testing splits\n",
        "train_set = PathMNIST(root=CFG.data_root, split=\"train\", transform=train_tfms, download=CFG.download, as_rgb=True)\n",
        "val_set   = PathMNIST(root=CFG.data_root, split=\"val\",   transform=eval_tfms, download=CFG.download, as_rgb=True)\n",
        "test_set  = PathMNIST(root=CFG.data_root, split=\"test\",  transform=eval_tfms, download=CFG.download, as_rgb=True)\n",
        "\n",
        "# create PyTorch DataLoaders\n",
        "train_loader = DataLoader(train_set, batch_size=CFG.batch_size, shuffle=True,  num_workers=CFG.workers, pin_memory=True)\n",
        "val_loader   = DataLoader(val_set,   batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.workers, pin_memory=True)\n",
        "test_loader  = DataLoader(test_set,  batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.workers, pin_memory=True)\n",
        "\n",
        "# print dataset sizes\n",
        "print(f\"Train/Val/Test sizes: {len(train_set)}/{len(val_set)}/{len(test_set)}\")\n",
        "\n",
        "# extract label names\n",
        "label_text = list(INFO[\"pathmnist\"][\"label\"].values())"
      ],
      "metadata": {
        "id": "HftBlFdeZRCO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model build (ResNet-50)\n",
        "def build_resnet50(num_classes: int, pretrained: bool, dropout: float = 0.0):\n",
        "    # select pretrained weights\n",
        "    weights = ResNet50_Weights.IMAGENET1K_V1 if pretrained\n",
        "    # load ResNet-50 model\n",
        "    model = resnet50(weights=weights)\n",
        "\n",
        "    # final fully-connected layer\n",
        "    in_features = model.fc.in_features\n",
        "    if dropout > 0:\n",
        "        model.fc = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(in_features, num_classes)\n",
        "        )\n",
        "    else:\n",
        "        # linear calssifier\n",
        "        model.fc = nn.Linear(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "# move to GPU\n",
        "model = build_resnet50(NUM_CLASSES, pretrained=CFG.use_pretrained, dropout=CFG.dropout).to(device)\n",
        "\n",
        "# optimiser + lr scheduler\n",
        "# Adam + weight decay\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
        "# no. of steps for warmup + cosine schedule\n",
        "steps_per_epoch = math.ceil(len(train_set) / CFG.batch_size)\n",
        "warmup_steps = CFG.warmup_epochs * steps_per_epoch\n",
        "total_steps  = CFG.epochs * steps_per_epoch\n",
        "\n",
        "def lr_lambda(step):\n",
        "    # warmup stage\n",
        "    if step < warmup_steps:\n",
        "        return (step + 1) / max(1, warmup_steps)\n",
        "    # cosine decay stage\n",
        "    progress = (step - warmup_steps) / max(1, (total_steps - warmup_steps))\n",
        "    return 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "# lr schedule\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=CFG.amp)\n",
        "\n",
        "# evaluation helper\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    all_targets, all_preds = [], []\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.squeeze().long().to(device, non_blocking=True)\n",
        "\n",
        "        # forward pass\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "\n",
        "        # tracking loss + accuracy\n",
        "        loss_sum += loss.item() * y.size(0)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "        # storing predictions for later evaluation metrics\n",
        "        all_targets.append(y.detach().cpu())\n",
        "        all_preds.append(pred.detach().cpu())\n",
        "\n",
        "    # compute accuracy + mean loss\n",
        "    acc = correct / total\n",
        "    y_true = torch.cat(all_targets).numpy()\n",
        "    y_pred = torch.cat(all_preds).numpy()\n",
        "    return acc, loss_sum / total, y_true, y_pred"
      ],
      "metadata": {
        "id": "AC_aLuxiZREP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "best_val = 0.0      # tracking best validation accuracy\n",
        "best_state = None   # storing best model checkpoint\n",
        "\n",
        "for epoch in range(CFG.epochs):\n",
        "    model.train()   # setting model to training mode\n",
        "    running = 0.0   # training loss\n",
        "\n",
        "    # progress bar\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CFG.epochs}\")\n",
        "    for x, y in pbar:\n",
        "        # move to GPU\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.squeeze().long().to(device, non_blocking=True)\n",
        "\n",
        "        # resetting gradients\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # forward + loss\n",
        "        with torch.cuda.amp.autocast(enabled=CFG.amp):\n",
        "            logits = model(x)\n",
        "            loss = F.cross_entropy(logits, y, label_smoothing=CFG.label_smoothing)\n",
        "        # backward + optimiser step\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # lr update\n",
        "        scheduler.step()\n",
        "        # tracking training loss for logging\n",
        "        running += loss.item() * y.size(0)\n",
        "        pbar.set_postfix(loss=f\"{loss.item():.4f}\", lr=f\"{optimizer.param_groups[0]['lr']:.2e}\")\n",
        "\n",
        "    # average training loss\n",
        "    train_loss = running / len(train_set)\n",
        "    # evaluate on validation set\n",
        "    val_acc, val_loss, _, _ = evaluate(model, val_loader)\n",
        "    print(f\"Epoch {epoch+1:03d}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc*100:.2f}%\")\n",
        "\n",
        "    # save model when validation accuracy increases\n",
        "    if val_acc > best_val:\n",
        "        best_val = val_acc\n",
        "        best_state = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"val_acc\": best_val,\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"cfg\": CFG.__dict__,\n",
        "        }\n",
        "        torch.save(best_state, CFG.ckpt_path)\n",
        "        print(f\"Saved best checkpoint: {CFG.ckpt_path} (val_acc={best_val*100:.2f}%)\")"
      ],
      "metadata": {
        "id": "v5piiQYRttge",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final test\n",
        "if os.path.isfile(CFG.ckpt_path):\n",
        "    # load checkpoint\n",
        "    ckpt = torch.load(CFG.ckpt_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model\"])  # load best model parameters\n",
        "    print(f\"Loaded best epoch: {ckpt.get('epoch', '?')}, best val_acc: {ckpt.get('val_acc', 0)*100:.2f}%\")\n",
        "\n",
        "# run evaluation on final test set\n",
        "test_acc, test_loss, y_true, y_pred = evaluate(model, test_loader)\n",
        "\n",
        "# print results\n",
        "print(f\"\\nTEST: loss={test_loss:.4f} | acc={test_acc*100:.2f}%\\n\")"
      ],
      "metadata": {
        "id": "hOmg0YTxZRGI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classification report\n",
        "print(classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "NUM_CLASSES = cm.shape[0]\n",
        "\n",
        "plt.figure(figsize=(10, 9))\n",
        "ax = sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,        # show counts\n",
        "    fmt=\"d\",           # integers\n",
        "    cmap=\"viridis\",\n",
        "    cbar=True,\n",
        "    square=True,\n",
        "    xticklabels=np.arange(NUM_CLASSES),\n",
        "    yticklabels=np.arange(NUM_CLASSES),\n",
        "    linewidths=0.5,\n",
        "    linecolor=\"white\"\n",
        ")\n",
        "ax.set_title(\"ResNet-50 - Confusion Matrix\")\n",
        "ax.set_xlabel(\"Predicted\")\n",
        "ax.set_ylabel(\"True\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bq2E2ocRYNyr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}