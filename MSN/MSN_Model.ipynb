{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "c1DsDfZtCLvv",
        "outputId": "b8c36891-f5d4-4687-efca-dfa481067053",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'msn'...\n",
            "remote: Enumerating objects: 69, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 69 (delta 4), reused 1 (delta 1), pack-reused 58 (from 1)\u001b[K\n",
            "Receiving objects: 100% (69/69), 219.49 KiB | 9.14 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n",
            "Collecting medmnist\n",
            "  Downloading medmnist-3.0.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from medmnist) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from medmnist) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from medmnist) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from medmnist) (0.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from medmnist) (4.67.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from medmnist) (11.3.0)\n",
            "Collecting fire (from medmnist)\n",
            "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from medmnist) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from medmnist) (0.23.0+cu126)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire->medmnist) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (1.16.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (3.5)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (2025.9.9)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->medmnist) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->medmnist) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->medmnist) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->medmnist) (3.0.2)\n",
            "Downloading medmnist-3.0.2-py3-none-any.whl (25 kB)\n",
            "Downloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fire, medmnist\n",
            "Successfully installed fire-0.7.1 medmnist-3.0.2\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/facebookresearch/msn.git\n",
        "!cd msn\n",
        "!pip install medmnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \"/content/msn/configs/pretrain/msn_vits16.yaml\"\n",
        "criterion:\n",
        "  ent_weight: 0.0               # weight for entropy term in the loss fxn\n",
        "  final_sharpen: 0.25           # sharpening factor applied to final output probabilities\n",
        "  me_max: true                  # mean entropy maximisation regulariser\n",
        "  memax_weight: 1.0             # weight of me-max\n",
        "  num_proto: 1024               # no. of prototype vectors (clusters)\n",
        "  start_sharpen: 0.25           # initial sharpening (at the start of training)\n",
        "  temperature: 0.1              # temp. param. for softmax\n",
        "  batch_size: 64                # batch size used to compute loss\n",
        "  use_ent: true                 # includes entropy term in loss\n",
        "  use_sinkhorn: true            # uses Sinkhorn-Knopp algorithm for balanced cluster assignments\n",
        "data:\n",
        "  color_jitter_strength: 0.5    # intensity of color jitter augmentation\n",
        "  pin_mem: true                 # uses pin memory for faster GPU training\n",
        "  num_workers: 6                # no. of CPU processes to use for data loading\n",
        "  image_folder: pathmnist       # folder containing image dataset\n",
        "  label_smoothing: 0.0          # amount of label smoothing for classification\n",
        "  patch_drop: 0.15              # prob. of dropping image patches\n",
        "  rand_size: 224                # size of random/global crops\n",
        "  focal_size: 96                # size of focal/local crops\n",
        "  rand_views: 1                 # no. of random/global views per image\n",
        "  focal_views: 10               # no. of focal/local views per image\n",
        "  root_path: null               # base path\n",
        "logging:\n",
        "  folder: logs/                    # directory where logs/results will be stored\n",
        "  write_tag: msn-pathmnist-train   # tag for this trial\n",
        "meta:\n",
        "  bottleneck: 1                   # no of bottleneck layers in projection head\n",
        "  copy_data: false                # copies dataset locally before training\n",
        "  drop_path_rate: 0.0             # prob. of dropping entire residual paths\n",
        "  hidden_dim: 2048                # hidden layer dim. in projection head\n",
        "  load_checkpoint: false          # loads model weight from saved cp\n",
        "  model_name: deit_small          # model architecture used (Data-efficient Image Transformer)\n",
        "  output_dim: 256                 # output embedding dim. (from projection head)\n",
        "  read_checkpoint: null           # path to cp file to load\n",
        "  use_bn: true                    # uses batch normalisation\n",
        "  use_fp16: true                  # mixed precision for faster training and lower memory usage\n",
        "  use_pred_head: true             # uses prediction head (false if only using encoder features)\n",
        "optimization:\n",
        "  clip_grad: 3.0                  # max. gradient norm for clipping\n",
        "  epochs: 100                     # no. of training epochs\n",
        "  final_lr: 1.0e-06               # learning rate (lr) after cosine decay schedule\n",
        "  final_weight_decay: 0.4         # weight decay (wd) after schedule\n",
        "  lr: 0.001                       # base lr\n",
        "  start_lr: 0.0002                # starting lr (for warmup phase)\n",
        "  warmup: 15                      # no. of warmup epochs\n",
        "  weight_decay: 0.04              # initial wd for regularisation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aApecitQ_fea",
        "outputId": "dca32628-a51c-463f-81fd-c7ffb9256a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/msn/configs/pretrain/msn_vits16.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \"/content/msn/configs/eval/lineval_msn_vits16.yaml\"\n",
        "meta:\n",
        "  model_name: deit_small\n",
        "  master_port: 8888\n",
        "  load_checkpoint: false\n",
        "  training: true\n",
        "  copy_data: false\n",
        "  device: cuda:0\n",
        "data:\n",
        "  root_path: null\n",
        "  image_folder: pathmnist\n",
        "  num_classes: 9\n",
        "  train_subset_frac: 1             # train_subset_frac was changed based on label %\n",
        "  train_subset_seed: 42\n",
        "optimization:\n",
        "  weight_decay: 0.0\n",
        "  lr: 3.0\n",
        "  epochs: 30\n",
        "  num_blocks: 1\n",
        "  normalize: true\n",
        "logging:\n",
        "  folder: logs_sub/\n",
        "  write_tag: msn-100-sub-linear    # write_tag was changed based on label %\n",
        "  pretrain_path: msn-pathmnist-train-latest.pth.tar"
      ],
      "metadata": {
        "id": "YYa7p23c1jnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \"/content/msn/src/data_manager.py\"\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the Creative Commons Attribution–NonCommercial 4.0 International License\n",
        "#\n",
        "# Modifications: Adapted for use in 'Self-supervised learning applied to unlabelled histopathology imaging data –\n",
        "# a comparison between Masked Siamese Networks and Self-Distillation with No Labels', 2025.\n",
        "#\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# ADAPTED FOR USE\n",
        "from medmnist import PathMNIST\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, Subset\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "from logging import getLogger\n",
        "\n",
        "from PIL import ImageFilter\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "_GLOBAL_SEED = 0        # setting global seed for reproducibility\n",
        "logger = getLogger()    # setting up logger object\n",
        "\n",
        "\n",
        "# ADAPTED FOR USE\n",
        "def _labels_from_wrapper(ds):\n",
        "    # works with PathMNISTWrapper before subsetting\n",
        "    if hasattr(ds, \"labels\"):\n",
        "        return np.asarray(ds.labels).squeeze()\n",
        "    if hasattr(ds, \"dataset\") and hasattr(ds.dataset, \"labels\"):\n",
        "        return np.asarray(ds.dataset.labels).squeeze()\n",
        "    raise RuntimeError(\"Could not find labels on dataset/wrapper for subsetting.\")\n",
        "\n",
        "# ADAPTED FOR USE\n",
        "def _balanced_indices(labels, per_class=100, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    labels = np.asarray(labels).squeeze()\n",
        "    idxs = []\n",
        "    for c in np.unique(labels):\n",
        "        cls = np.where(labels == c)[0]\n",
        "        take = min(per_class, len(cls))\n",
        "        idxs.extend(rng.choice(cls, size=take, replace=False))\n",
        "    rng.shuffle(idxs)\n",
        "    return list(map(int, idxs))\n",
        "\n",
        "# ADAPTED FOR USE\n",
        "def init_data(          # defining function to prepare DataLoader for dataset\n",
        "    transform,          # data preprocesing and augmentation\n",
        "    batch_size,         # no. of batches\n",
        "    pin_mem=True,       # uses pin memory for faster GPU training\n",
        "    num_workers=6,      # no. of CPU processes to use for data loading\n",
        "    world_size=1,       # no. of distributed processes\n",
        "    rank=0,             # ID of current process (0 = main process)\n",
        "    root_path=None,     # base path\n",
        "    image_folder=None,  # dataset name\n",
        "    training=True,      # loading training, val or test split?\n",
        "    copy_data=False,    # copies dataset locally before training\n",
        "    drop_last=True,     # drop last batch due to incompleteness\n",
        "    # choosing explicit split when not training; and training label budget\n",
        "    split=None,                         # \"train\" | \"val\" | \"test\"\n",
        "    train_subset_frac=None,             # 0.1 for 10% labels\n",
        "    train_subset_size=None,             # put number of samples\n",
        "    train_subset_per_class=None,        # default 100 per class\n",
        "    train_subset_seed=42,               # reproducibility\n",
        "):\n",
        "    if image_folder == \"pathmnist\":   # creating instance of PathMNIST dataset\n",
        "      # choose split\n",
        "      split_name = split if split is not None else ('train' if training else 'test')\n",
        "      dataset = PathMNISTWrapper(split=split_name, transform=transform)\n",
        "\n",
        "      # train label-budget (few-label linear eval); only when training on labels\n",
        "      if training and any(x is not None for x in (train_subset_frac, train_subset_size, train_subset_per_class)):\n",
        "          labels = _labels_from_wrapper(dataset)\n",
        "          N = len(labels)\n",
        "          rng = np.random.default_rng(int(train_subset_seed))\n",
        "          if train_subset_per_class is not None:\n",
        "              idx = _balanced_indices(labels, per_class=int(train_subset_per_class), seed=int(train_subset_seed))\n",
        "          elif train_subset_size is not None:\n",
        "              k = max(1, min(int(train_subset_size), N))\n",
        "              idx = list(rng.choice(np.arange(N), size=k, replace=False))\n",
        "          else:  # frac\n",
        "              k = max(1, int(round(float(train_subset_frac) * N)))\n",
        "              idx = list(rng.choice(np.arange(N), size=k, replace=False))\n",
        "          dataset = Subset(dataset, idx)\n",
        "          logger.info(f\"pathmnist TRAIN subset enabled: using {len(idx)} labeled samples \"\n",
        "                      f\"(mode={'per-class' if train_subset_per_class is not None else 'size' if train_subset_size is not None else 'frac'})\")\n",
        "\n",
        "\n",
        "    # creates distributed sampler that ensures that each process gets a\n",
        "    # different subset of the data\n",
        "    dist_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        dataset=dataset,\n",
        "        num_replicas=world_size,    # total no. of processes\n",
        "        rank=rank)                  # rank (ID) of the current process\n",
        "    # building DataLoader to feed data into the model\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        sampler=dist_sampler,       # handles data partitioning across GPUs\n",
        "        batch_size=batch_size,      # no. of samples per batch\n",
        "        drop_last=drop_last,        # decided whether to drop last batch\n",
        "        pin_memory=pin_mem,         # speeds up data transfer to CUDA\n",
        "        num_workers=num_workers,    # no. of subprocesses for loading data\n",
        "        )\n",
        "    # logs that DataLoader has been set up successfully\n",
        "    logger.info('pathmnist unsupervised data loader created')\n",
        "\n",
        "    # returns DataLoader and its associated distributed sampler\n",
        "    return (data_loader, dist_sampler)\n",
        "\n",
        "\n",
        "def make_transforms(\n",
        "    rand_size=224,      # random views (global crops) - 224x224\n",
        "    focal_size=96,      # focal views (local crops) - 96x96\n",
        "    rand_crop_scale=(0.3, 1.0),     # global crop is 30%-100% of orignal area\n",
        "    focal_crop_scale=(0.05, 0.3),   # local crop is 5%-30% of original area\n",
        "    color_jitter=1.0,\n",
        "    rand_views=2,       # no. of global views per sample\n",
        "    focal_views=10,     # no. of focal views per sample\n",
        "):\n",
        "    # prints message to logger that the transforms are being built\n",
        "    logger.info('making pathmnist data transforms')\n",
        "\n",
        "    def get_color_distortion(s=1.0):\n",
        "        # s is the strength of color distortion.\n",
        "        color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n",
        "        # applies jitter with 80% probability\n",
        "        rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
        "        # converts to grayscale with 20% probability\n",
        "        rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
        "        color_distort = transforms.Compose([\n",
        "            rnd_color_jitter,\n",
        "            rnd_gray])\n",
        "        return color_distort\n",
        "\n",
        "    rand_transform = transforms.Compose([\n",
        "        # randomly crops and resizes image\n",
        "        transforms.RandomResizedCrop(rand_size, scale=rand_crop_scale),\n",
        "        # flips the image horizontally with 50% probability\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        # applies color jitter and grayscale\n",
        "        get_color_distortion(s=color_jitter),\n",
        "        # blurs image with 50% probability\n",
        "        GaussianBlur(p=0.5),\n",
        "        # converts PIL image to PyTorch tensor\n",
        "        transforms.ToTensor(),\n",
        "        # applies mean and std normalisation\n",
        "        # ADAPTED FOR USE\n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    focal_transform = transforms.Compose([\n",
        "        # randomly crops and resizes image\n",
        "        transforms.RandomResizedCrop(focal_size, scale=focal_crop_scale),\n",
        "        # flips the image horizontally with 50% probability\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        # applies color jitter and grayscale\n",
        "        get_color_distortion(s=color_jitter),\n",
        "        # blurs image with 50% probability\n",
        "        GaussianBlur(p=0.5),\n",
        "        # converts PIL image to PyTorch tensor\n",
        "        transforms.ToTensor(),\n",
        "        # applies mean and std normalisation\n",
        "        # ADAPTED FOR USE\n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    # creates MultiViewTransform object that will\n",
        "    # apply rand_transform for rand_views times\n",
        "    # apply focal_transform for focal_views times\n",
        "    transform = MultiViewTransform(\n",
        "        rand_transform=rand_transform,\n",
        "        focal_transform=focal_transform,\n",
        "        rand_views=rand_views,\n",
        "        focal_views=focal_views\n",
        "    )\n",
        "    return transform\n",
        "\n",
        "\n",
        "# creates multiple augmented views (random and focal) of a single input image\n",
        "class MultiViewTransform(object):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        rand_transform=None,\n",
        "        focal_transform=None,\n",
        "        rand_views=1,         # no. of gloabl views to be generated per image\n",
        "        focal_views=1,        # no. of local views to be generated per image\n",
        "    ):\n",
        "        self.rand_views = rand_views\n",
        "        self.focal_views = focal_views\n",
        "        self.rand_transform = rand_transform\n",
        "        self.focal_transform = focal_transform\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img_views = []        # initialising empty list to store the\n",
        "                              # augmented views\n",
        "\n",
        "        # -- generate random views + adds them to the list\n",
        "        if self.rand_views > 0:\n",
        "            img_views += [self.rand_transform(img) for i in range(self.rand_views)]\n",
        "\n",
        "        # -- generate focal views + adds them to the list\n",
        "        if self.focal_views > 0:\n",
        "            img_views += [self.focal_transform(img) for i in range(self.focal_views)]\n",
        "\n",
        "        # returns a list of all transformed views of the image\n",
        "        return img_views\n",
        "\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    def __init__(self, p=0.5, radius_min=0.1, radius_max=2.):\n",
        "        self.prob = p       # probability of applying the blur\n",
        "        self.radius_min = radius_min    # min blur radius\n",
        "        self.radius_max = radius_max    # max blur radius\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # returns 1 with prob p, returns 0 with prob 1-p\n",
        "        if torch.bernoulli(torch.tensor(self.prob)) == 0:\n",
        "            return img\n",
        "\n",
        "        radius = self.radius_min + torch.rand(1) * (self.radius_max - self.radius_min)\n",
        "        # applies the blur using the randomly sampled radius above, with range\n",
        "        # [radius_min, radius_max]\n",
        "        return img.filter(ImageFilter.GaussianBlur(radius=radius.item()))\n",
        "\n",
        "# ADAPTED FOR USE\n",
        "class PathMNISTWrapper(Dataset):\n",
        "  def __init__(self, split='train', transform=None):\n",
        "    self.dataset = PathMNIST(split=split, download=True)\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)    # returns no. of samples in pathmnist dataset\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img, label = self.dataset[idx]  # retrieves the image and label at index idx\n",
        "    img = self.transform(img)\n",
        "    # make label a scalar int\n",
        "    y = int(label) if isinstance(label, (int, np.integer)) else int(label[0])\n",
        "    return img, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vg_LHXr44CN",
        "outputId": "02905129-f5e5-4d8b-a7fc-f09a72342e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/msn/src/data_manager.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \"/content/msn/src/deit.py\"\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\")\n",
        "#\n",
        "\n",
        "\"\"\"\n",
        "Mostly copy-paste from timm library.\n",
        "https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
        "\"\"\"\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from src.utils import trunc_normal_\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"\n",
        "    3x3 Convolution stems for ViT following ViTC models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3,\n",
        "                               stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n",
        "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., norm_layer=nn.LayerNorm,\n",
        "                 conv_stem=False, conv_stem_channels=None, conv_stem_strides=None, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "\n",
        "        if conv_stem:\n",
        "            self.patch_embed = ConvEmbed(conv_stem_channels, conv_stem_strides,\n",
        "                                         in_chans=in_chans, img_size=img_size)\n",
        "        else:\n",
        "            self.patch_embed = PatchEmbed(\n",
        "                img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "        # Classifier head\n",
        "        self.fc = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "        self.pred = None\n",
        "\n",
        "        trunc_normal_(self.pos_embed, std=.02)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "            if isinstance(m, nn.Conv2d) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, return_before_head=False, patch_drop=0.):\n",
        "        if not isinstance(x, list):\n",
        "            x = [x]\n",
        "        idx_crops = torch.cumsum(torch.unique_consecutive(\n",
        "            torch.tensor([inp.shape[-1] for inp in x]),\n",
        "            return_counts=True,\n",
        "        )[1], 0)\n",
        "        start_idx = 0\n",
        "        for end_idx in idx_crops:\n",
        "            _h = self.forward_features(torch.cat(x[start_idx:end_idx]), patch_drop)\n",
        "            _z = self.forward_head(_h)\n",
        "            if start_idx == 0:\n",
        "                h, z = _h, _z\n",
        "            else:\n",
        "                h, z = torch.cat((h, _h)), torch.cat((z, _z))\n",
        "            patch_drop = 0.\n",
        "            start_idx = end_idx\n",
        "\n",
        "        if return_before_head:\n",
        "            return h, z\n",
        "        return z\n",
        "\n",
        "    def forward_head(self, x):\n",
        "        if self.pred is not None:\n",
        "            return self.pred(x)\n",
        "        return x\n",
        "\n",
        "    def forward_features(self, x, patch_drop):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        if patch_drop > 0:\n",
        "            patch_keep = 1. - patch_drop\n",
        "            T_H = int(np.floor((x.shape[1]-1)*patch_keep))\n",
        "            perm = 1 + torch.randperm(x.shape[1]-1)[:T_H]  # keep class token\n",
        "            idx = torch.cat([torch.zeros(1, dtype=perm.dtype, device=perm.device), perm])\n",
        "            x = x[:, idx, :]\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "        x = x[:, 0]\n",
        "        if self.fc is not None:\n",
        "            x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def forward_blocks(self, x, num_blocks=1, patch_drop=0.):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        if patch_drop > 0:\n",
        "            patch_keep = 1. - patch_drop\n",
        "            T_H = int(np.floor((x.shape[1]-1)*patch_keep))\n",
        "            perm = 1 + torch.randperm(x.shape[1]-1)[:T_H]  # keep class token\n",
        "            idx = torch.cat([torch.zeros(1, dtype=perm.dtype, device=perm.device), perm])\n",
        "            x = x[:, idx, :]\n",
        "\n",
        "        cls_x = []\n",
        "        for i in range(len(self.blocks)):\n",
        "            x = self.blocks[i](x)\n",
        "            if (len(self.blocks) - i) <= num_blocks:\n",
        "                cls_x.append(x[:, 0])\n",
        "\n",
        "        return torch.cat(cls_x, dim=-1)\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N:\n",
        "            return pos_embed\n",
        "        class_emb = pos_embed[:, 0]\n",
        "        pos_embed = pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(\n",
        "            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
        "            scale_factor=math.sqrt(npatch / N),\n",
        "            mode='bicubic',\n",
        "        )\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "    def forward_selfattention(self, x):\n",
        "        B, nc, w, h = x.shape\n",
        "        N = self.pos_embed.shape[1] - 1\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # interpolate patch embeddings\n",
        "        dim = x.shape[-1]\n",
        "        w0 = w // self.patch_embed.patch_size\n",
        "        h0 = h // self.patch_embed.patch_size\n",
        "        class_pos_embed = self.pos_embed[:, 0]\n",
        "        patch_pos_embed = self.pos_embed[:, 1:]\n",
        "        patch_pos_embed = nn.functional.interpolate(\n",
        "            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
        "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
        "            mode='bicubic',\n",
        "        )\n",
        "        # sometimes there is a floating point error in the interpolation and so\n",
        "        # we need to pad the patch positional encoding.\n",
        "        if w0 != patch_pos_embed.shape[-2]:\n",
        "            helper = torch.zeros(h0)[None, None, None, :].repeat(1, dim, w0 - patch_pos_embed.shape[-2], 1).to(x.device)\n",
        "            patch_pos_embed = torch.cat((patch_pos_embed, helper), dim=-2)\n",
        "        if h0 != patch_pos_embed.shape[-1]:\n",
        "            helper = torch.zeros(w0)[None, None, :, None].repeat(1, dim, 1, h0 - patch_pos_embed.shape[-1]).to(x.device)\n",
        "            patch_pos_embed = torch.cat((patch_pos_embed, helper), dim=-1)\n",
        "\n",
        "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        pos_embed = torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            if i < len(self.blocks) - 1:\n",
        "                x = blk(x)\n",
        "            else:\n",
        "                return blk(x, return_attention=True)\n",
        "\n",
        "    def forward_return_n_last_blocks(self, x, n=1, return_patch_avgpool=False):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        # we will return the [CLS] tokens from the `n` last blocks\n",
        "        output = []\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "            if len(self.blocks) - i <= n:\n",
        "                output.append(self.norm(x)[:, 0])\n",
        "        if return_patch_avgpool:\n",
        "            x = self.norm(x)\n",
        "            # In addition to the [CLS] tokens from the `n` last blocks, we also return\n",
        "            # the patch tokens from the last block. This is useful for linear eval.\n",
        "            output.append(torch.mean(x[:, 1:], dim=1))\n",
        "        return torch.cat(output, dim=-1)\n",
        "\n",
        "\n",
        "def deit_tiny(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_small(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_small_p8(patch_size=8, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_small_p7(patch_size=7, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vitc_4gf(patch_size=16, **kwargs):\n",
        "    channels = [48, 96, 192, 384, 384]\n",
        "    strides = [2, 2, 2, 2, 1]\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=384, depth=11, num_heads=6, mlp_ratio=3,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
        "        conv_stem=True, conv_stem_channels=channels, conv_stem_strides=strides,\n",
        "        **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_small_convstem(patch_size=16, **kwargs):\n",
        "    channels = [48, 96, 192, 384, 384]\n",
        "    strides = [2, 2, 2, 2, 1]\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=384, depth=11, num_heads=6, mlp_ratio=6,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
        "        conv_stem=True, conv_stem_channels=channels, conv_stem_strides=strides,\n",
        "        **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_base_p8(patch_size=8, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_base_p7(patch_size=7, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_base_p4(patch_size=4, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_base(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_large_p7(patch_size=7, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_large_p8(patch_size=8, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_large(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_huge(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_huge_p8(patch_size=8, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_huge_p7(patch_size=7, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_huge_p10(patch_size=10, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTv5ulKOpPsm",
        "outputId": "f182bc6c-8eae-4c49-8a89-5a06af4fcd56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/msn/src/data_manager.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \"/content/msn/src/losses.py\"\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the Creative Commons Attribution–NonCommercial 4.0 International License\n",
        "#\n",
        "\n",
        "from logging import getLogger\n",
        "import torch\n",
        "import math\n",
        "from src.utils import AllReduce\n",
        "\n",
        "\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "def init_msn_loss(\n",
        "    num_views=1,\n",
        "    tau=0.1,\n",
        "    me_max=True,\n",
        "    return_preds=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Make unsupervised MSN loss\n",
        "\n",
        "    :num_views: number of anchor views\n",
        "    :param tau: cosine similarity temperature\n",
        "    :param me_max: whether to perform me-max regularization\n",
        "    :param return_preds: whether to return anchor predictions\n",
        "    \"\"\"\n",
        "    softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    def sharpen(p, T):\n",
        "        sharp_p = p**(1./T)\n",
        "        sharp_p /= torch.sum(sharp_p, dim=1, keepdim=True)\n",
        "        return sharp_p\n",
        "\n",
        "    def snn(query, supports, support_labels, temp=tau):\n",
        "        \"\"\" Soft Nearest Neighbours similarity classifier \"\"\"\n",
        "        query = torch.nn.functional.normalize(query)\n",
        "        supports = torch.nn.functional.normalize(supports)\n",
        "        return softmax(query @ supports.T / temp) @ support_labels\n",
        "\n",
        "    def loss(\n",
        "        anchor_views,\n",
        "        target_views,\n",
        "        prototypes,\n",
        "        proto_labels,\n",
        "        T=0.25,\n",
        "        use_entropy=False,\n",
        "        use_sinkhorn=False,\n",
        "        sharpen=sharpen,\n",
        "        snn=snn\n",
        "    ):\n",
        "        # Step 1: compute anchor predictions\n",
        "        probs = snn(anchor_views, prototypes, proto_labels)\n",
        "\n",
        "        # Step 2: compute targets for anchor predictions\n",
        "        with torch.no_grad():\n",
        "            targets = sharpen(snn(target_views, prototypes, proto_labels), T=T)\n",
        "            if use_sinkhorn:\n",
        "                targets = distributed_sinkhorn(targets)\n",
        "            targets = torch.cat([targets for _ in range(num_views)], dim=0)\n",
        "\n",
        "        # Step 3: compute cross-entropy loss H(targets, queries)\n",
        "        loss = torch.mean(torch.sum(torch.log(probs**(-targets)), dim=1))\n",
        "\n",
        "        # Step 4: compute me-max regularizer\n",
        "        rloss = 0.\n",
        "        if me_max:\n",
        "            avg_probs = AllReduce.apply(torch.mean(probs, dim=0))\n",
        "            rloss = - torch.sum(torch.log(avg_probs**(-avg_probs))) + math.log(float(len(avg_probs)))\n",
        "\n",
        "        sloss = 0.\n",
        "        if use_entropy:\n",
        "            sloss = torch.mean(torch.sum(torch.log(probs**(-probs)), dim=1))\n",
        "\n",
        "        # -- logging\n",
        "        with torch.no_grad():\n",
        "            num_ps = float(len(set(targets.argmax(dim=1).tolist())))\n",
        "            max_t = targets.max(dim=1).values.mean()\n",
        "            min_t = targets.min(dim=1).values.mean()\n",
        "            log_dct = {'np': num_ps, 'max_t': max_t, 'min_t': min_t}\n",
        "\n",
        "        if return_preds:\n",
        "            return loss, rloss, sloss, log_dct, targets\n",
        "\n",
        "        return loss, rloss, sloss, log_dct\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def distributed_sinkhorn(Q, num_itr=3, use_dist=True):\n",
        "    _got_dist = use_dist and torch.distributed.is_available() \\\n",
        "        and torch.distributed.is_initialized() \\\n",
        "        and (torch.distributed.get_world_size() > 1)\n",
        "\n",
        "    if _got_dist:\n",
        "        world_size = torch.distributed.get_world_size()\n",
        "    else:\n",
        "        world_size = 1\n",
        "\n",
        "    Q = Q.T\n",
        "    B = Q.shape[1] * world_size  # number of samples to assign\n",
        "    K = Q.shape[0]  # how many prototypes\n",
        "\n",
        "    # make the matrix sums to 1\n",
        "    sum_Q = torch.sum(Q)\n",
        "    if _got_dist:\n",
        "        torch.distributed.all_reduce(sum_Q)\n",
        "    Q /= sum_Q\n",
        "\n",
        "    for it in range(num_itr):\n",
        "        # normalize each row: total weight per prototype must be 1/K\n",
        "        sum_of_rows = torch.sum(Q, dim=1, keepdim=True)\n",
        "        if _got_dist:\n",
        "            torch.distributed.all_reduce(sum_of_rows)\n",
        "        Q /= sum_of_rows\n",
        "        Q /= K\n",
        "\n",
        "        # normalize each column: total weight per sample must be 1/B\n",
        "        Q /= torch.sum(Q, dim=0, keepdim=True)\n",
        "        Q /= B\n",
        "\n",
        "    Q *= B  # the colomns must sum to 1 so that Q is an assignment\n",
        "    return Q.T"
      ],
      "metadata": {
        "id": "chDiElEJtwV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \"/content/msn/src/msn_train.py\"\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the Creative Commons Attribution–NonCommercial 4.0 International License\n",
        "#\n",
        "\n",
        "import os\n",
        "\n",
        "# -- FOR DISTRIBUTED TRAINING ENSURE ONLY 1 DEVICE VISIBLE PER PROCESS\n",
        "try:\n",
        "    # -- WARNING: IF DOING DISTRIBUTED TRAINING ON A NON-SLURM CLUSTER, MAKE\n",
        "    # --          SURE TO UPDATE THIS TO GET LOCAL-RANK ON NODE, OR ENSURE\n",
        "    # --          THAT YOUR JOBS ARE LAUNCHED WITH ONLY 1 DEVICE VISIBLE\n",
        "    # --          TO EACH PROCESS\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['SLURM_LOCALID']\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import sys\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "import src.deit as deit\n",
        "from src.utils import (\n",
        "    AllReduceSum,\n",
        "    trunc_normal_,\n",
        "    gpu_timer,\n",
        "    init_distributed,\n",
        "    WarmupCosineSchedule,\n",
        "    CosineWDSchedule,\n",
        "    CSVLogger,\n",
        "    grad_logger,\n",
        "    AverageMeter\n",
        ")\n",
        "from src.losses import init_msn_loss\n",
        "from src.data_manager import (\n",
        "    init_data,\n",
        "    make_transforms\n",
        ")\n",
        "\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "# --\n",
        "log_timings = True\n",
        "log_freq = 10\n",
        "checkpoint_freq = 25\n",
        "checkpoint_freq_itr = 2500\n",
        "# --\n",
        "\n",
        "_GLOBAL_SEED = 0\n",
        "np.random.seed(_GLOBAL_SEED)\n",
        "torch.manual_seed(_GLOBAL_SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logger = logging.getLogger()\n",
        "\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    # ----------------------------------------------------------------------- #\n",
        "    #  PASSED IN PARAMS FROM CONFIG FILE\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    # -- META\n",
        "    model_name = args['meta']['model_name']\n",
        "    two_layer = False if 'two_layer' not in args['meta'] else args['meta']['two_layer']\n",
        "    bottleneck = 1 if 'bottleneck' not in args['meta'] else args['meta']['bottleneck']\n",
        "    output_dim = args['meta']['output_dim']\n",
        "    hidden_dim = args['meta']['hidden_dim']\n",
        "    load_model = args['meta']['load_checkpoint']\n",
        "    r_file = args['meta']['read_checkpoint']\n",
        "    copy_data = args['meta']['copy_data']\n",
        "    use_pred_head = args['meta']['use_pred_head']\n",
        "    use_bn = args['meta']['use_bn']\n",
        "    drop_path_rate = args['meta']['drop_path_rate']\n",
        "    if not torch.cuda.is_available():\n",
        "        device = torch.device('cpu')\n",
        "    else:\n",
        "        device = torch.device('cuda:0')\n",
        "        torch.cuda.set_device(device)\n",
        "\n",
        "    # -- CRITERTION\n",
        "    memax_weight = 1 if 'memax_weight' not in args['criterion'] else args['criterion']['memax_weight']\n",
        "    ent_weight = 1 if 'ent_weight' not in args['criterion'] else args['criterion']['ent_weight']\n",
        "    freeze_proto = False if 'freeze_proto' not in args['criterion'] else args['criterion']['freeze_proto']\n",
        "    use_ent = False if 'use_ent' not in args['criterion'] else args['criterion']['use_ent']\n",
        "    reg = args['criterion']['me_max']\n",
        "    use_sinkhorn = args['criterion']['use_sinkhorn']\n",
        "    num_proto = args['criterion']['num_proto']\n",
        "    # --\n",
        "    batch_size = args['criterion']['batch_size']\n",
        "    temperature = args['criterion']['temperature']\n",
        "    _start_T = args['criterion']['start_sharpen']\n",
        "    _final_T = args['criterion']['final_sharpen']\n",
        "\n",
        "    # -- DATA\n",
        "    label_smoothing = args['data']['label_smoothing']\n",
        "    pin_mem = False if 'pin_mem' not in args['data'] else args['data']['pin_mem']\n",
        "    num_workers = 1 if 'num_workers' not in args['data'] else args['data']['num_workers']\n",
        "    color_jitter = args['data']['color_jitter_strength']\n",
        "    root_path = args['data']['root_path']\n",
        "    image_folder = args['data']['image_folder']\n",
        "    patch_drop = args['data']['patch_drop']\n",
        "    rand_size = args['data']['rand_size']\n",
        "    rand_views = args['data']['rand_views']\n",
        "    focal_views = args['data']['focal_views']\n",
        "    focal_size = args['data']['focal_size']\n",
        "    # --\n",
        "\n",
        "    # -- OPTIMIZATION\n",
        "    clip_grad = args['optimization']['clip_grad']\n",
        "    wd = float(args['optimization']['weight_decay'])\n",
        "    final_wd = float(args['optimization']['final_weight_decay'])\n",
        "    num_epochs = args['optimization']['epochs']\n",
        "    warmup = args['optimization']['warmup']\n",
        "    start_lr = args['optimization']['start_lr']\n",
        "    lr = args['optimization']['lr']\n",
        "    final_lr = args['optimization']['final_lr']\n",
        "\n",
        "    # -- LOGGING\n",
        "    folder = args['logging']['folder']\n",
        "    tag = args['logging']['write_tag']\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    try:\n",
        "        mp.set_start_method('spawn')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # -- init torch distributed backend\n",
        "    world_size, rank = init_distributed()\n",
        "    logger.info(f'Initialized (rank/world-size) {rank}/{world_size}')\n",
        "    # if rank > 0:\n",
        "    #     logger.setLevel(logging.ERROR)\n",
        "\n",
        "    # -- proto details\n",
        "    assert num_proto > 0, 'unsupervised pre-training requires specifying prototypes'\n",
        "\n",
        "    # -- log/checkpointing paths\n",
        "    log_file = os.path.join(folder, f'{tag}_r{rank}.csv')\n",
        "    save_path = os.path.join(folder, f'{tag}' + '-ep{epoch}.pth.tar')\n",
        "    latest_path = os.path.join(folder, f'{tag}-latest.pth.tar')\n",
        "    load_path = None\n",
        "    if load_model:\n",
        "        load_path = os.path.join(folder, r_file) if r_file is not None else latest_path\n",
        "\n",
        "    # -- make csv_logger\n",
        "    csv_logger = CSVLogger(log_file,\n",
        "                           ('%d', 'epoch'),\n",
        "                           ('%d', 'itr'),\n",
        "                           ('%.5f', 'msn'),\n",
        "                           ('%.5f', 'me_max'),\n",
        "                           ('%.5f', 'ent'),\n",
        "                           ('%d', 'time (ms)'))\n",
        "\n",
        "    # -- init model\n",
        "    encoder = init_model(\n",
        "        device=device,\n",
        "        model_name=model_name,\n",
        "        two_layer=two_layer,\n",
        "        use_pred=use_pred_head,\n",
        "        use_bn=use_bn,\n",
        "        bottleneck=bottleneck,\n",
        "        hidden_dim=hidden_dim,\n",
        "        output_dim=output_dim,\n",
        "        drop_path_rate=drop_path_rate)\n",
        "    target_encoder = copy.deepcopy(encoder)\n",
        "    if (world_size > 1):\n",
        "        encoder = torch.nn.SyncBatchNorm.convert_sync_batchnorm(encoder)\n",
        "        target_encoder = torch.nn.SyncBatchNorm.convert_sync_batchnorm(target_encoder)\n",
        "\n",
        "    # -- init losses\n",
        "    msn = init_msn_loss(\n",
        "        num_views=focal_views+rand_views,\n",
        "        tau=temperature,\n",
        "        me_max=reg,\n",
        "        return_preds=True)\n",
        "\n",
        "    def one_hot(targets, num_classes, smoothing=label_smoothing):\n",
        "        off_value = smoothing / num_classes\n",
        "        on_value = 1. - smoothing + off_value\n",
        "        targets = targets.long().view(-1, 1).to(device)\n",
        "        return torch.full((len(targets), num_classes), off_value, device=device).scatter_(1, targets, on_value)\n",
        "\n",
        "    # -- make data transforms\n",
        "    transform = make_transforms(\n",
        "        rand_size=rand_size,\n",
        "        focal_size=focal_size,\n",
        "        rand_views=rand_views+1,\n",
        "        focal_views=focal_views,\n",
        "        color_jitter=color_jitter)\n",
        "\n",
        "    # -- init data-loaders/samplers\n",
        "    (unsupervised_loader,\n",
        "     unsupervised_sampler) = init_data(\n",
        "         transform=transform,\n",
        "         batch_size=batch_size,\n",
        "         pin_mem=pin_mem,\n",
        "         num_workers=num_workers,\n",
        "         world_size=world_size,\n",
        "         rank=rank,\n",
        "         root_path=root_path,\n",
        "         image_folder=image_folder,\n",
        "         training=True,\n",
        "         copy_data=copy_data)\n",
        "    ipe = len(unsupervised_loader)\n",
        "    logger.info(f'iterations per epoch: {ipe}')\n",
        "\n",
        "    # -- make prototypes\n",
        "    prototypes, proto_labels = None, None\n",
        "    if num_proto > 0:\n",
        "        with torch.no_grad():\n",
        "            prototypes = torch.empty(num_proto, output_dim)\n",
        "            _sqrt_k = (1./output_dim)**0.5\n",
        "            torch.nn.init.uniform_(prototypes, -_sqrt_k, _sqrt_k)\n",
        "            prototypes = torch.nn.parameter.Parameter(prototypes).to(device)\n",
        "\n",
        "            # -- init prototype labels\n",
        "            proto_labels = one_hot(torch.tensor([i for i in range(num_proto)]), num_proto)\n",
        "\n",
        "        if not freeze_proto:\n",
        "            prototypes.requires_grad = True\n",
        "        logger.info(f'Created prototypes: {prototypes.shape}')\n",
        "        logger.info(f'Requires grad: {prototypes.requires_grad}')\n",
        "\n",
        "    # -- init optimizer and scheduler\n",
        "    encoder, optimizer, scheduler, wd_scheduler = init_opt(\n",
        "        encoder=encoder,\n",
        "        prototypes=prototypes,\n",
        "        wd=wd,\n",
        "        final_wd=final_wd,\n",
        "        start_lr=start_lr,\n",
        "        ref_lr=lr,\n",
        "        final_lr=final_lr,\n",
        "        iterations_per_epoch=ipe,\n",
        "        warmup=warmup,\n",
        "        num_epochs=num_epochs)\n",
        "    if world_size > 1:\n",
        "        encoder = DistributedDataParallel(encoder)\n",
        "        target_encoder = DistributedDataParallel(target_encoder)\n",
        "        for p in target_encoder.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    # -- momentum schedule\n",
        "    _start_m, _final_m = 0.996, 1.0\n",
        "    _increment = (_final_m - _start_m) / (ipe * num_epochs * 1.25)\n",
        "    momentum_scheduler = (_start_m + (_increment*i) for i in range(int(ipe*num_epochs*1.25)+1))\n",
        "\n",
        "    # -- sharpening schedule\n",
        "    _increment_T = (_final_T - _start_T) / (ipe * num_epochs * 1.25)\n",
        "    sharpen_scheduler = (_start_T + (_increment_T*i) for i in range(int(ipe*num_epochs*1.25)+1))\n",
        "\n",
        "    start_epoch = 0\n",
        "    # -- load training checkpoint\n",
        "    if load_model:\n",
        "        encoder, target_encoder, prototypes, optimizer, start_epoch = load_checkpoint(\n",
        "            device=device,\n",
        "            prototypes=prototypes,\n",
        "            r_path=load_path,\n",
        "            encoder=encoder,\n",
        "            target_encoder=target_encoder,\n",
        "            opt=optimizer)\n",
        "        for _ in range(start_epoch*ipe):\n",
        "            scheduler.step()\n",
        "            wd_scheduler.step()\n",
        "            next(momentum_scheduler)\n",
        "            next(sharpen_scheduler)\n",
        "\n",
        "    def save_checkpoint(epoch):\n",
        "\n",
        "        if target_encoder is not None:\n",
        "            target_encoder_state_dict = target_encoder.state_dict()\n",
        "        else:\n",
        "            target_encoder_state_dict = None\n",
        "\n",
        "        save_dict = {\n",
        "            'encoder': encoder.state_dict(),\n",
        "            'opt': optimizer.state_dict(),\n",
        "            'prototypes': prototypes.data,\n",
        "            'target_encoder': target_encoder_state_dict,\n",
        "            'epoch': epoch,\n",
        "            'loss': loss_meter.avg,\n",
        "            'batch_size': batch_size,\n",
        "            'world_size': world_size,\n",
        "            'lr': lr,\n",
        "            'temperature': temperature\n",
        "        }\n",
        "        if rank == 0:\n",
        "            torch.save(save_dict, latest_path)\n",
        "            if (epoch + 1) % checkpoint_freq == 0 \\\n",
        "                    or (epoch + 1) % 10 == 0 and epoch < checkpoint_freq:\n",
        "                torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))\n",
        "\n",
        "    # -- TRAINING LOOP\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        logger.info('Epoch %d' % (epoch + 1))\n",
        "\n",
        "        # -- update distributed-data-loader epoch\n",
        "        unsupervised_sampler.set_epoch(epoch)\n",
        "\n",
        "        loss_meter = AverageMeter()\n",
        "        ploss_meter = AverageMeter()\n",
        "        rloss_meter = AverageMeter()\n",
        "        eloss_meter = AverageMeter()\n",
        "        np_meter = AverageMeter()\n",
        "        maxp_meter = AverageMeter()\n",
        "        time_meter = AverageMeter()\n",
        "        data_meter = AverageMeter()\n",
        "\n",
        "        for itr, (udata, _) in enumerate(unsupervised_loader):\n",
        "\n",
        "            def load_imgs():\n",
        "                # -- unsupervised imgs\n",
        "                imgs = [u.to(device, non_blocking=True) for u in udata]\n",
        "                return imgs\n",
        "            imgs, dtime = gpu_timer(load_imgs)\n",
        "            data_meter.update(dtime)\n",
        "\n",
        "            def train_step():\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # --\n",
        "                # h: representations of 'imgs' before head\n",
        "                # z: representations of 'imgs' after head\n",
        "                # -- If use_pred_head=False, then encoder.pred (prediction\n",
        "                #    head) is None, and _forward_head just returns the\n",
        "                #    identity, z=h\n",
        "                h, z = encoder(imgs[1:], return_before_head=True, patch_drop=patch_drop)\n",
        "                with torch.no_grad():\n",
        "                    h, _ = target_encoder(imgs[0], return_before_head=True)\n",
        "\n",
        "                # Step 1. convert representations to fp32\n",
        "                h, z = h.float(), z.float()\n",
        "\n",
        "                # Step 2. determine anchor views/supports and their\n",
        "                #         corresponding target views/supports\n",
        "                # --\n",
        "                anchor_views, target_views = z, h.detach()\n",
        "                T = next(sharpen_scheduler)\n",
        "\n",
        "                # Step 3. compute msn loss with me-max regularization\n",
        "                (ploss, me_max, ent, logs, _) = msn(\n",
        "                    T=T,\n",
        "                    use_sinkhorn=use_sinkhorn,\n",
        "                    use_entropy=use_ent,\n",
        "                    anchor_views=anchor_views,\n",
        "                    target_views=target_views,\n",
        "                    proto_labels=proto_labels,\n",
        "                    prototypes=prototypes)\n",
        "                loss = ploss + memax_weight*me_max + ent_weight*ent\n",
        "\n",
        "                _new_lr = scheduler.step()\n",
        "                _new_wd = wd_scheduler.step()\n",
        "                # --\n",
        "\n",
        "                # Step 4. Optimization step\n",
        "                loss.backward()\n",
        "                with torch.no_grad():\n",
        "                    prototypes.grad.data = AllReduceSum.apply(prototypes.grad.data)\n",
        "                grad_stats = grad_logger(encoder.named_parameters())\n",
        "                if clip_grad > 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip_grad)\n",
        "                optimizer.step()\n",
        "\n",
        "                # Step 5. momentum update of target encoder\n",
        "                with torch.no_grad():\n",
        "                    m = next(momentum_scheduler)\n",
        "                    for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                        param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "                return (float(loss), float(ploss), float(me_max), float(ent),\n",
        "                        logs, _new_lr, _new_wd, grad_stats)\n",
        "            (loss, ploss, rloss, eloss,\n",
        "             _logs, _new_lr, _new_wd, grad_stats), etime = gpu_timer(train_step)\n",
        "            loss_meter.update(loss)\n",
        "            ploss_meter.update(ploss)\n",
        "            rloss_meter.update(rloss)\n",
        "            eloss_meter.update(eloss)\n",
        "            maxp_meter.update(_logs['max_t'])\n",
        "            np_meter.update(_logs['np'])\n",
        "\n",
        "            time_meter.update(etime)\n",
        "\n",
        "            # -- Save Checkpoint\n",
        "            if itr % checkpoint_freq_itr == 0:\n",
        "                save_checkpoint(epoch)\n",
        "\n",
        "            # -- Logging\n",
        "            def log_stats():\n",
        "                csv_logger.log(epoch + 1, itr, ploss, rloss, eloss, etime)\n",
        "                if (itr % log_freq == 0) or np.isnan(loss) or np.isinf(loss):\n",
        "                    logger.info('[%d, %5d] loss: %.3f (%.3f %.3f %.3f) '\n",
        "                                '(np: %.1f, max-t: %.3f) '\n",
        "                                '[wd: %.2e] [lr: %.2e] '\n",
        "                                '[mem: %.2e] '\n",
        "                                '(%d ms; %d ms)'\n",
        "                                % (epoch + 1, itr,\n",
        "                                   loss_meter.avg,\n",
        "                                   ploss_meter.avg,\n",
        "                                   rloss_meter.avg,\n",
        "                                   eloss_meter.avg,\n",
        "                                   np_meter.avg,\n",
        "                                   maxp_meter.avg,\n",
        "                                   _new_wd,\n",
        "                                   _new_lr,\n",
        "                                   torch.cuda.max_memory_allocated() / 1024.**2,\n",
        "                                   time_meter.avg,\n",
        "                                   data_meter.avg))\n",
        "\n",
        "                    if grad_stats is not None:\n",
        "                        logger.info('[%d, %5d] grad_stats: [%.2e %.2e] (%.2e, %.2e)'\n",
        "                                    % (epoch + 1, itr,\n",
        "                                       grad_stats.first_layer,\n",
        "                                       grad_stats.last_layer,\n",
        "                                       grad_stats.min,\n",
        "                                       grad_stats.max))\n",
        "            log_stats()\n",
        "            assert not np.isnan(loss), 'loss is nan'\n",
        "\n",
        "        # -- Save Checkpoint after every epoch\n",
        "        logger.info('avg. loss %.3f' % loss_meter.avg)\n",
        "        save_checkpoint(epoch+1)\n",
        "\n",
        "\n",
        "def load_checkpoint(\n",
        "    device,\n",
        "    r_path,\n",
        "    prototypes,\n",
        "    encoder,\n",
        "    target_encoder,\n",
        "    opt\n",
        "):\n",
        "    checkpoint = torch.load(r_path, map_location=torch.device('cpu'))\n",
        "    epoch = checkpoint['epoch']\n",
        "\n",
        "    # -- loading encoder\n",
        "    pretrained_dict = checkpoint['encoder']\n",
        "    if ('scaling_module.bias' not in pretrained_dict) and ('scaling_bias' in pretrained_dict):\n",
        "        pretrained_dict['scaling_module.bias'] = pretrained_dict['scaling_bias']\n",
        "        del pretrained_dict['scaling_bias']\n",
        "    msg = encoder.load_state_dict(pretrained_dict)\n",
        "    logger.info(f'loaded pretrained encoder from epoch {epoch} with msg: {msg}')\n",
        "\n",
        "    # -- loading target_encoder\n",
        "    if target_encoder is not None:\n",
        "        print(list(checkpoint.keys()))\n",
        "        pretrained_dict = checkpoint['target_encoder']\n",
        "        if ('scaling_module.bias' not in pretrained_dict) and ('scaling_bias' in pretrained_dict):\n",
        "            pretrained_dict['scaling_module.bias'] = pretrained_dict['scaling_bias']\n",
        "            del pretrained_dict['scaling_bias']\n",
        "        msg = target_encoder.load_state_dict(pretrained_dict)\n",
        "        logger.info(f'loaded pretrained encoder from epoch {epoch} with msg: {msg}')\n",
        "\n",
        "    # -- loading prototypes\n",
        "    if (prototypes is not None) and ('prototypes' in checkpoint):\n",
        "        with torch.no_grad():\n",
        "            prototypes.data = checkpoint['prototypes'].to(device)\n",
        "        logger.info(f'loaded prototypes from epoch {epoch}')\n",
        "\n",
        "    # -- loading optimizer\n",
        "    opt.load_state_dict(checkpoint['opt'])\n",
        "    logger.info(f'loaded optimizers from epoch {epoch}')\n",
        "    logger.info(f'read-path: {r_path}')\n",
        "    del checkpoint\n",
        "    return encoder, target_encoder, prototypes, opt, epoch\n",
        "\n",
        "\n",
        "def init_model(\n",
        "    device,\n",
        "    model_name='resnet50',\n",
        "    use_pred=False,\n",
        "    use_bn=False,\n",
        "    two_layer=False,\n",
        "    bottleneck=1,\n",
        "    hidden_dim=2048,\n",
        "    output_dim=128,\n",
        "    drop_path_rate=0.1,\n",
        "):\n",
        "    encoder = deit.__dict__[model_name](drop_path_rate=drop_path_rate)\n",
        "    emb_dim = 192 if 'tiny' in model_name else 384 if 'small' in model_name else 768 if 'base' in model_name else 1024 if 'large' in model_name else 1280\n",
        "\n",
        "    # -- projection head\n",
        "    encoder.fc = None\n",
        "    fc = OrderedDict([])\n",
        "    fc['fc1'] = torch.nn.Linear(emb_dim, hidden_dim)\n",
        "    if use_bn:\n",
        "        fc['bn1'] = torch.nn.BatchNorm1d(hidden_dim)\n",
        "    fc['gelu1'] = torch.nn.GELU()\n",
        "    fc['fc2'] = torch.nn.Linear(hidden_dim, hidden_dim)\n",
        "    if use_bn:\n",
        "        fc['bn2'] = torch.nn.BatchNorm1d(hidden_dim)\n",
        "    fc['gelu2'] = torch.nn.GELU()\n",
        "    fc['fc3'] = torch.nn.Linear(hidden_dim, output_dim)\n",
        "    encoder.fc = torch.nn.Sequential(fc)\n",
        "\n",
        "    for m in encoder.modules():\n",
        "        if isinstance(m, torch.nn.Linear):\n",
        "            trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                torch.nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, torch.nn.LayerNorm):\n",
        "            torch.nn.init.constant_(m.bias, 0)\n",
        "            torch.nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    encoder.to(device)\n",
        "    logger.info(encoder)\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def init_opt(\n",
        "    encoder,\n",
        "    iterations_per_epoch,\n",
        "    start_lr,\n",
        "    ref_lr,\n",
        "    warmup,\n",
        "    num_epochs,\n",
        "    prototypes=None,\n",
        "    wd=1e-6,\n",
        "    final_wd=1e-6,\n",
        "    final_lr=0.0\n",
        "):\n",
        "    param_groups = [\n",
        "        {'params': (p for n, p in encoder.named_parameters()\n",
        "                    if ('bias' not in n) and ('bn' not in n) and len(p.shape) != 1)},\n",
        "        {'params': (p for n, p in encoder.named_parameters()\n",
        "                    if ('bias' in n) or ('bn' in n) or (len(p.shape) == 1)),\n",
        "         'WD_exclude': True,\n",
        "         'weight_decay': 0}\n",
        "    ]\n",
        "    if prototypes is not None:\n",
        "        param_groups.append({\n",
        "            'params': [prototypes],\n",
        "            'lr': ref_lr,\n",
        "            'LARS_exclude': True,\n",
        "            'WD_exclude': True,\n",
        "            'weight_decay': 0\n",
        "        })\n",
        "\n",
        "    logger.info('Using AdamW')\n",
        "    optimizer = torch.optim.AdamW(param_groups)\n",
        "    scheduler = WarmupCosineSchedule(\n",
        "        optimizer,\n",
        "        warmup_steps=int(warmup*iterations_per_epoch),\n",
        "        start_lr=start_lr,\n",
        "        ref_lr=ref_lr,\n",
        "        final_lr=final_lr,\n",
        "        T_max=int(1.25*num_epochs*iterations_per_epoch))\n",
        "    wd_scheduler = CosineWDSchedule(\n",
        "        optimizer,\n",
        "        ref_wd=wd,\n",
        "        final_wd=final_wd,\n",
        "        T_max=int(1.25*num_epochs*iterations_per_epoch))\n",
        "    return encoder, optimizer, scheduler, wd_scheduler\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "xRiQFI2_twX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \"/content/msn/src/sgd.py\"\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the Creative Commons Attribution–NonCommercial 4.0 International License\n",
        "#\n",
        "\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "\n",
        "class SGD(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr, momentum=0, weight_decay=0, nesterov=False):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(f'Invalid learning rate: {lr}')\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(f'Invalid weight_decay value: {weight_decay}')\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay,\n",
        "                        nesterov=nesterov)\n",
        "        super(SGD, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            nesterov = group['nesterov']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                d_p = p.grad\n",
        "                if weight_decay != 0:\n",
        "                    d_p = d_p.add(p, alpha=weight_decay)\n",
        "                d_p.mul_(-group['lr'])\n",
        "\n",
        "                if momentum != 0:\n",
        "                    param_state = self.state[p]\n",
        "                    if 'momentum_buffer' not in param_state:\n",
        "                        buf = param_state['momentum_buffer'] = d_p.clone().detach()\n",
        "                    else:\n",
        "                        buf = param_state['momentum_buffer']\n",
        "                        buf.mul_(momentum).add_(d_p)\n",
        "\n",
        "                    if nesterov:\n",
        "                        d_p.add_(buf, alpha=momentum)\n",
        "                    else:\n",
        "                        d_p = buf\n",
        "\n",
        "                p.add_(d_p)\n",
        "\n",
        "        return None"
      ],
      "metadata": {
        "id": "E-7KcZMkSmgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \"/content/msn/src/utils.py\"\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the Creative Commons Attribution–NonCommercial 4.0 International License\n",
        "#\n",
        "\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "from logging import getLogger\n",
        "\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "def gpu_timer(closure, log_timings=True):\n",
        "    \"\"\" Helper to time gpu-time to execute closure() \"\"\"\n",
        "    elapsed_time = -1.\n",
        "    if log_timings:\n",
        "        start = torch.cuda.Event(enable_timing=True)\n",
        "        end = torch.cuda.Event(enable_timing=True)\n",
        "        start.record()\n",
        "\n",
        "    result = closure()\n",
        "\n",
        "    if log_timings:\n",
        "        end.record()\n",
        "        torch.cuda.synchronize()\n",
        "        elapsed_time = start.elapsed_time(end)\n",
        "\n",
        "    return result, elapsed_time\n",
        "\n",
        "\n",
        "def init_distributed(port=40111, rank_and_world_size=(None, None)):\n",
        "\n",
        "    if dist.is_available() and dist.is_initialized():\n",
        "        return dist.get_world_size(), dist.get_rank()\n",
        "\n",
        "    rank, world_size = rank_and_world_size\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "\n",
        "    if (rank is None) or (world_size is None):\n",
        "        try:\n",
        "            world_size = int(os.environ['SLURM_NTASKS'])\n",
        "            rank = int(os.environ['SLURM_PROCID'])\n",
        "            os.environ['MASTER_ADDR'] = os.environ['HOSTNAME']\n",
        "        except Exception:\n",
        "            logger.info('SLURM vars not set (distributed training not available)')\n",
        "            world_size, rank = 1, 0\n",
        "            return world_size, rank\n",
        "\n",
        "    try:\n",
        "        os.environ['MASTER_PORT'] = str(port)\n",
        "        torch.distributed.init_process_group(\n",
        "            backend='nccl',\n",
        "            world_size=world_size,\n",
        "            rank=rank)\n",
        "    except Exception:\n",
        "        world_size, rank = 1, 0\n",
        "        logger.info('distributed training not available')\n",
        "\n",
        "    return world_size, rank\n",
        "\n",
        "\n",
        "class WarmupCosineSchedule(object):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        optimizer,\n",
        "        warmup_steps,\n",
        "        start_lr,\n",
        "        ref_lr,\n",
        "        T_max,\n",
        "        last_epoch=-1,\n",
        "        final_lr=0.\n",
        "    ):\n",
        "        self.optimizer = optimizer\n",
        "        self.start_lr = start_lr\n",
        "        self.ref_lr = ref_lr\n",
        "        self.final_lr = final_lr\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.T_max = T_max - warmup_steps\n",
        "        self._step = 0.\n",
        "\n",
        "    def step(self):\n",
        "        self._step += 1\n",
        "        if self._step < self.warmup_steps:\n",
        "            progress = float(self._step) / float(max(1, self.warmup_steps))\n",
        "            new_lr = self.start_lr + progress * (self.ref_lr - self.start_lr)\n",
        "        else:\n",
        "            # -- progress after warmup\n",
        "            progress = float(self._step - self.warmup_steps) / float(max(1, self.T_max))\n",
        "            new_lr = max(self.final_lr,\n",
        "                         self.final_lr + (self.ref_lr - self.final_lr) * 0.5 * (1. + math.cos(math.pi * progress)))\n",
        "\n",
        "        for group in self.optimizer.param_groups:\n",
        "            group['lr'] = new_lr\n",
        "\n",
        "        return new_lr\n",
        "\n",
        "\n",
        "class CosineWDSchedule(object):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        optimizer,\n",
        "        ref_wd,\n",
        "        T_max,\n",
        "        final_wd=0.\n",
        "    ):\n",
        "        self.optimizer = optimizer\n",
        "        self.ref_wd = ref_wd\n",
        "        self.final_wd = final_wd\n",
        "        self.T_max = T_max\n",
        "        self._step = 0.\n",
        "\n",
        "    def step(self):\n",
        "        self._step += 1\n",
        "        progress = self._step / self.T_max\n",
        "        new_wd = self.final_wd + (self.ref_wd - self.final_wd) * 0.5 * (1. + math.cos(math.pi * progress))\n",
        "\n",
        "        if self.final_wd <= self.ref_wd:\n",
        "            new_wd = max(self.final_wd, new_wd)\n",
        "        else:\n",
        "            new_wd = min(self.final_wd, new_wd)\n",
        "\n",
        "        for group in self.optimizer.param_groups:\n",
        "            if ('WD_exclude' not in group) or not group['WD_exclude']:\n",
        "                group['weight_decay'] = new_wd\n",
        "        return new_wd\n",
        "\n",
        "\n",
        "class CSVLogger(object):\n",
        "\n",
        "    def __init__(self, fname, *argv):\n",
        "        self.fname = fname\n",
        "        self.types = []\n",
        "        # -- print headers\n",
        "        with open(self.fname, '+a') as f:\n",
        "            for i, v in enumerate(argv, 1):\n",
        "                self.types.append(v[0])\n",
        "                if i < len(argv):\n",
        "                    print(v[1], end=',', file=f)\n",
        "                else:\n",
        "                    print(v[1], end='\\n', file=f)\n",
        "\n",
        "    def log(self, *argv):\n",
        "        with open(self.fname, '+a') as f:\n",
        "            for i, tv in enumerate(zip(self.types, argv), 1):\n",
        "                end = ',' if i < len(argv) else '\\n'\n",
        "                print(tv[0] % tv[1], end=end, file=f)\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.max = float('-inf')\n",
        "        self.min = float('inf')\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.max = max(val, self.max)\n",
        "        self.min = min(val, self.min)\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "class AllGather(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        if (\n",
        "            dist.is_available()\n",
        "            and dist.is_initialized()\n",
        "            and (dist.get_world_size() > 1)\n",
        "        ):\n",
        "            outputs = [torch.zeros_like(x) for _ in range(dist.get_world_size())]\n",
        "            dist.all_gather(outputs, x)\n",
        "            return torch.cat(outputs, 0)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grads):\n",
        "        if (\n",
        "            dist.is_available()\n",
        "            and dist.is_initialized()\n",
        "            and (dist.get_world_size() > 1)\n",
        "        ):\n",
        "            s = (grads.shape[0] // dist.get_world_size()) * dist.get_rank()\n",
        "            e = (grads.shape[0] // dist.get_world_size()) * (dist.get_rank() + 1)\n",
        "            grads = grads.contiguous()\n",
        "            dist.all_reduce(grads)\n",
        "            return grads[s:e]\n",
        "        return grads\n",
        "\n",
        "\n",
        "class AllReduceSum(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        if (\n",
        "            dist.is_available()\n",
        "            and dist.is_initialized()\n",
        "            and (dist.get_world_size() > 1)\n",
        "        ):\n",
        "            x = x.contiguous()\n",
        "            dist.all_reduce(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grads):\n",
        "        return grads\n",
        "\n",
        "\n",
        "class AllReduce(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        if (\n",
        "            dist.is_available()\n",
        "            and dist.is_initialized()\n",
        "            and (dist.get_world_size() > 1)\n",
        "        ):\n",
        "            x = x.contiguous() / dist.get_world_size()\n",
        "            dist.all_reduce(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grads):\n",
        "        return grads\n",
        "\n",
        "\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # type: (Tensor, float, float, float, float) -> Tensor\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "\n",
        "def grad_logger(named_params):\n",
        "    stats = AverageMeter()\n",
        "    stats.first_layer = None\n",
        "    stats.last_layer = None\n",
        "    for n, p in named_params:\n",
        "        if (p.grad is not None) and not (n.endswith('.bias') or len(p.shape) == 1):\n",
        "            grad_norm = float(torch.norm(p.grad.data))\n",
        "            stats.update(grad_norm)\n",
        "            if 'qkv' in n:\n",
        "                stats.last_layer = grad_norm\n",
        "                if stats.first_layer is None:\n",
        "                    stats.first_layer = grad_norm\n",
        "    if stats.first_layer is None or stats.last_layer is None:\n",
        "        stats.first_layer = stats.last_layer = 0.\n",
        "    return stats"
      ],
      "metadata": {
        "id": "zxBSYy1ISvOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \"/content/msn/linear_eval.py\"\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the Creative Commons Attribution–NonCommercial 4.0 International License\n",
        "#\n",
        "# Modifications: Adapted for use in 'Self-supervised learning applied to unlabelled histopathology imaging data –\n",
        "# a comparison between Masked Siamese Networks and Self-Distillation with No Labels', 2025.\n",
        "#\n",
        "\n",
        "import os\n",
        "\n",
        "from torch.nn.modules.module import T\n",
        "\n",
        "# -- FOR DISTRIBUTED TRAINING ENSURE ONLY 1 DEVICE VISIBLE PER PROCESS\n",
        "try:\n",
        "    # -- WARNING: IF DOING DISTRIBUTED TRAINING ON A NON-SLURM CLUSTER, MAKE\n",
        "    # --          SURE TO UPDATE THIS TO GET LOCAL-RANK ON NODE, OR ENSURE\n",
        "    # --          THAT YOUR JOBS ARE LAUNCHED WITH ONLY 1 DEVICE VISIBLE\n",
        "    # --          TO EACH PROCESS\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['SLURM_LOCALID']\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "import argparse\n",
        "import yaml\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import src.deit as deit\n",
        "from src.utils import (\n",
        "    AllReduce,\n",
        "    init_distributed,\n",
        "    WarmupCosineSchedule\n",
        ")\n",
        "from src.data_manager import init_data\n",
        "from src.sgd import SGD\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "# --\n",
        "log_timings = True\n",
        "log_freq = 10\n",
        "checkpoint_freq = 50\n",
        "# --\n",
        "\n",
        "_GLOBAL_SEED = 0\n",
        "np.random.seed(_GLOBAL_SEED)\n",
        "torch.manual_seed(_GLOBAL_SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logger = logging.getLogger()\n",
        "\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    # -- META\n",
        "    model_name = args['meta']['model_name']\n",
        "    port = args['meta']['master_port']\n",
        "    load_checkpoint = args['meta']['load_checkpoint']\n",
        "    training = args['meta']['training']\n",
        "    copy_data = args['meta']['copy_data']\n",
        "    device = torch.device(args['meta']['device'])\n",
        "    if 'cuda' in args['meta']['device']:\n",
        "        torch.cuda.set_device(device)\n",
        "\n",
        "    # -- DATA\n",
        "    root_path = args['data']['root_path']\n",
        "    image_folder = args['data']['image_folder']\n",
        "    num_classes = args['data']['num_classes']\n",
        "\n",
        "    # -- OPTIMIZATION\n",
        "    wd = float(args['optimization']['weight_decay'])\n",
        "    ref_lr = args['optimization']['lr']\n",
        "    num_epochs = args['optimization']['epochs']\n",
        "    num_blocks = args['optimization']['num_blocks']\n",
        "    l2_normalize = args['optimization']['normalize']\n",
        "\n",
        "    # -- LOGGING\n",
        "    folder = args['logging']['folder']\n",
        "    tag = args['logging']['write_tag']\n",
        "    r_file_enc = args['logging']['pretrain_path']\n",
        "\n",
        "    # -- log/checkpointing paths\n",
        "    r_enc_path = os.path.join(folder, r_file_enc)\n",
        "    w_enc_path = os.path.join(folder, f'{tag}-lin-eval.pth.tar')\n",
        "\n",
        "    # -- init distributed\n",
        "    world_size, rank = init_distributed(port)\n",
        "    logger.info(f'initialized rank/world-size: {rank}/{world_size}')\n",
        "\n",
        "    # -- optimization/evaluation params\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "    if training:\n",
        "        batch_size = 256\n",
        "    else:\n",
        "        batch_size = 128\n",
        "        load_checkpoint = True\n",
        "        num_epochs = 1\n",
        "\n",
        "    # -- init loss\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # ADAPTED FOR USE\n",
        "    # -- make train data transforms and data loaders/samples\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            (0.5, 0.5, 0.5),\n",
        "            (0.5, 0.5, 0.5))])\n",
        "    # ADAPTED FOR USE\n",
        "    data_loader, dist_sampler = init_data(\n",
        "        transform=transform,\n",
        "        batch_size=batch_size,\n",
        "        world_size=world_size,\n",
        "        rank=rank,\n",
        "        root_path=root_path,\n",
        "        image_folder=image_folder,\n",
        "        training=training,\n",
        "        copy_data=copy_data,\n",
        "        num_workers=6,\n",
        "        drop_last=False,\n",
        "        # label budget (read from your YAML under data:)\n",
        "        train_subset_frac=args['data'].get('train_subset_frac', None),\n",
        "        train_subset_size=args['data'].get('train_subset_size', None),\n",
        "        train_subset_per_class=args['data'].get('train_subset_per_class', None),\n",
        "        train_subset_seed=args['data'].get('train_subset_seed', 42)\n",
        "        )\n",
        "\n",
        "    ipe = len(data_loader)\n",
        "    logger.info(f'initialized data-loader (ipe {ipe})')\n",
        "\n",
        "    # ADAPTED FOR USE\n",
        "    # -- make val data transforms and data loaders/samples\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            (0.5, 0.5, 0.5),\n",
        "            (0.5, 0.5, 0.5))])\n",
        "    val_data_loader, val_dist_sampler = init_data(\n",
        "        transform=val_transform,\n",
        "        batch_size=batch_size,\n",
        "        world_size=world_size,\n",
        "        rank=rank,\n",
        "        root_path=root_path,\n",
        "        image_folder=image_folder,\n",
        "        training=False,\n",
        "        drop_last=False,\n",
        "        copy_data=copy_data,\n",
        "        split='test',\n",
        "        num_workers=6\n",
        "        )\n",
        "    logger.info(f'initialized val data-loader (ipe {len(val_data_loader)})')\n",
        "\n",
        "    # -- init model and optimizer\n",
        "    encoder, linear_classifier, optimizer, scheduler = init_model(\n",
        "        device=device,\n",
        "        device_str=args['meta']['device'],\n",
        "        num_classes=num_classes,\n",
        "        num_blocks=num_blocks,\n",
        "        normalize=l2_normalize,\n",
        "        training=training,\n",
        "        r_enc_path=r_enc_path,\n",
        "        iterations_per_epoch=ipe,\n",
        "        world_size=world_size,\n",
        "        ref_lr=ref_lr,\n",
        "        weight_decay=wd,\n",
        "        num_epochs=num_epochs,\n",
        "        model_name=model_name)\n",
        "    logger.info(encoder)\n",
        "\n",
        "    best_acc = None\n",
        "    start_epoch = 0\n",
        "    # -- load checkpoint\n",
        "    if not training or load_checkpoint:\n",
        "        encoder, linear_classifier, optimizer, scheduler, start_epoch, best_acc = load_from_path(\n",
        "            r_path=w_enc_path,\n",
        "            encoder=encoder,\n",
        "            linear_classifier=linear_classifier,\n",
        "            opt=optimizer,\n",
        "            sched=scheduler,\n",
        "            device_str=args['meta']['device'])\n",
        "    if not training:\n",
        "        logger.info('putting model in eval mode')\n",
        "        encoder.eval()\n",
        "        logger.info(sum(p.numel() for n, p in encoder.named_parameters()\n",
        "                        if p.requires_grad and ('fc' not in n)))\n",
        "        start_epoch = 0\n",
        "\n",
        "    encoder.eval()\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "\n",
        "        def train_step():\n",
        "            # -- update distributed-data-loader epoch\n",
        "            dist_sampler.set_epoch(epoch)\n",
        "            top1_correct, top5_correct, total = 0, 0, 0\n",
        "            for i, data in enumerate(data_loader):\n",
        "                with torch.cuda.amp.autocast(enabled=True):\n",
        "                    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "                    with torch.no_grad():\n",
        "                        outputs = encoder.forward_blocks(inputs, num_blocks)\n",
        "                outputs = linear_classifier(outputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total += inputs.shape[0]\n",
        "                top5_correct += float(outputs.topk(5, dim=1).indices.eq(labels.unsqueeze(1)).sum())\n",
        "                top1_correct += float(outputs.max(dim=1).indices.eq(labels).sum())\n",
        "                top1_acc = 100. * top1_correct / total\n",
        "                top5_acc = 100. * top5_correct / total\n",
        "                if training:\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "                if i % log_freq == 0:\n",
        "                    logger.info('[%d, %5d] %.3f%% %.3f%% (loss: %.3f)'\n",
        "                                % (epoch + 1, i, top1_acc, top5_acc, loss))\n",
        "            return 100. * top1_correct / total\n",
        "\n",
        "        def val_step():\n",
        "            top1_correct, total = 0, 0\n",
        "            for i, data in enumerate(val_data_loader):\n",
        "                with torch.cuda.amp.autocast(enabled=True):\n",
        "                    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "                    outputs = encoder.forward_blocks(inputs, num_blocks)\n",
        "                outputs = linear_classifier(outputs)\n",
        "                total += inputs.shape[0]\n",
        "                top1_correct += outputs.max(dim=1).indices.eq(labels).sum()\n",
        "                top1_acc = 100. * top1_correct / total\n",
        "\n",
        "            top1_acc = AllReduce.apply(top1_acc)\n",
        "            logger.info('[%d, %5d] %.3f%%' % (epoch + 1, i, top1_acc))\n",
        "            return top1_acc\n",
        "\n",
        "        train_top1 = 0.\n",
        "        train_top1 = train_step()\n",
        "        with torch.no_grad():\n",
        "            val_top1 = val_step()\n",
        "\n",
        "        log_str = 'train:' if training else 'test:'\n",
        "        logger.info('[%d] (%s %.3f%%) (val: %.3f%%)'\n",
        "                    % (epoch + 1, log_str, train_top1, val_top1))\n",
        "\n",
        "        # -- logging/checkpointing\n",
        "        if training and (rank == 0) and ((best_acc is None) or (best_acc < val_top1)):\n",
        "            best_acc = val_top1\n",
        "            save_dict = {\n",
        "                'target_encoder': encoder.state_dict(),\n",
        "                'classifier': linear_classifier.state_dict(),\n",
        "                'opt': optimizer.state_dict(),\n",
        "                'epoch': epoch + 1,\n",
        "                'world_size': world_size,\n",
        "                'best_top1_acc': best_acc,\n",
        "                'batch_size': batch_size,\n",
        "                'lr': ref_lr,\n",
        "            }\n",
        "            torch.save(save_dict, w_enc_path)\n",
        "\n",
        "    return train_top1, val_top1\n",
        "\n",
        "\n",
        "class LinearClassifier(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_labels=9, normalize=True):\n",
        "        super(LinearClassifier, self).__init__()\n",
        "        self.normalize = normalize\n",
        "        self.norm = torch.nn.LayerNorm(dim)\n",
        "        self.linear = torch.nn.Linear(dim, num_labels)\n",
        "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
        "        self.linear.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # flatten\n",
        "        x = self.norm(x)\n",
        "        if self.normalize:\n",
        "            x = torch.nn.functional.normalize(x)\n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "def load_pretrained(\n",
        "    r_path,\n",
        "    encoder,\n",
        "    linear_classifier,\n",
        "    device_str\n",
        "):\n",
        "    checkpoint = torch.load(r_path, map_location='cpu')\n",
        "    pretrained_dict = {k.replace('module.', ''): v for k, v in checkpoint['target_encoder'].items()}\n",
        "    for k, v in encoder.state_dict().items():\n",
        "        if k not in pretrained_dict:\n",
        "            logger.info(f'key \"{k}\" could not be found in loaded state dict')\n",
        "        elif pretrained_dict[k].shape != v.shape:\n",
        "            logger.info(f'key \"{k}\" is of different shape in model and loaded state dict')\n",
        "            pretrained_dict[k] = v\n",
        "    msg = encoder.load_state_dict(pretrained_dict, strict=False)\n",
        "    logger.info(f'loaded pretrained model with msg: {msg}')\n",
        "    logger.info(f'loaded pretrained encoder from epoch: {checkpoint[\"epoch\"]} '\n",
        "                f'path: {r_path}')\n",
        "\n",
        "    if linear_classifier is not None:\n",
        "        pretrained_dict = {k.replace('module.', ''): v for k, v in checkpoint['classifier'].items()}\n",
        "        for k, v in linear_classifier.state_dict().items():\n",
        "            if k not in pretrained_dict:\n",
        "                logger.info(f'key \"{k}\" could not be found in loaded state dict')\n",
        "            elif pretrained_dict[k].shape != v.shape:\n",
        "                logger.info(f'key \"{k}\" is of different shape in model and loaded state dict')\n",
        "                pretrained_dict[k] = v\n",
        "        msg = linear_classifier.load_state_dict(pretrained_dict, strict=False)\n",
        "        logger.info(f'loaded pretrained model with msg: {msg}')\n",
        "        logger.info(f'loaded pretrained encoder from epoch: {checkpoint[\"epoch\"]} '\n",
        "                    f'path: {r_path}')\n",
        "\n",
        "    del checkpoint\n",
        "    return encoder, linear_classifier\n",
        "\n",
        "\n",
        "def load_from_path(\n",
        "    r_path,\n",
        "    encoder,\n",
        "    linear_classifier,\n",
        "    opt,\n",
        "    sched,\n",
        "    device_str\n",
        "):\n",
        "    encoder, linear_classifier = load_pretrained(r_path, encoder, linear_classifier, device_str)\n",
        "    checkpoint = torch.load(r_path, map_location=device_str)\n",
        "\n",
        "    best_acc = None\n",
        "    if 'best_top1_acc' in checkpoint:\n",
        "        best_acc = checkpoint['best_top1_acc']\n",
        "\n",
        "    epoch = checkpoint['epoch']\n",
        "    if opt is not None:\n",
        "        opt.load_state_dict(checkpoint['opt'])\n",
        "        sched.load_state_dict(checkpoint['sched'])\n",
        "        logger.info(f'loaded optimizers from epoch {epoch}')\n",
        "    logger.info(f'read-path: {r_path}')\n",
        "    del checkpoint\n",
        "    return encoder, opt, sched, epoch, best_acc\n",
        "\n",
        "\n",
        "def init_model(\n",
        "    device,\n",
        "    device_str,\n",
        "    num_classes,\n",
        "    num_blocks,\n",
        "    training,\n",
        "    r_enc_path,\n",
        "    iterations_per_epoch,\n",
        "    world_size,\n",
        "    ref_lr,\n",
        "    num_epochs,\n",
        "    normalize,\n",
        "    model_name='deit_small',\n",
        "    warmup_epochs=0,\n",
        "    weight_decay=0\n",
        "):\n",
        "    # -- init model\n",
        "    encoder = deit.__dict__[model_name]()\n",
        "    emb_dim = 192 if 'tiny' in model_name else 384 if 'small' in model_name else 768 if 'base' in model_name else 1024 if 'large' in model_name else 1280\n",
        "    emb_dim *= num_blocks\n",
        "    encoder.fc = None\n",
        "    encoder.norm = None\n",
        "\n",
        "    encoder.to(device)\n",
        "    encoder, _ = load_pretrained(\n",
        "        r_path=r_enc_path,\n",
        "        encoder=encoder,\n",
        "        linear_classifier=None,\n",
        "        device_str=device_str)\n",
        "\n",
        "    linear_classifier = LinearClassifier(emb_dim, num_classes, normalize).to(device)\n",
        "\n",
        "    # -- init optimizer\n",
        "    optimizer, scheduler = None, None\n",
        "    param_groups = [\n",
        "        {'params': (p for n, p in linear_classifier.named_parameters()\n",
        "                    if ('bias' not in n) and ('bn' not in n) and len(p.shape) != 1)},\n",
        "        {'params': (p for n, p in linear_classifier.named_parameters()\n",
        "                    if ('bias' in n) or ('bn' in n) or (len(p.shape) == 1)),\n",
        "         'weight_decay': 0}\n",
        "    ]\n",
        "    optimizer = SGD(\n",
        "        param_groups,\n",
        "        nesterov=True,\n",
        "        weight_decay=weight_decay,\n",
        "        momentum=0.9,\n",
        "        lr=ref_lr)\n",
        "    scheduler = WarmupCosineSchedule(\n",
        "        optimizer,\n",
        "        warmup_steps=warmup_epochs*iterations_per_epoch,\n",
        "        start_lr=ref_lr,\n",
        "        ref_lr=ref_lr,\n",
        "        T_max=num_epochs*iterations_per_epoch)\n",
        "    if world_size > 1:\n",
        "        linear_classifier = DistributedDataParallel(linear_classifier)\n",
        "\n",
        "    return encoder, linear_classifier, optimizer, scheduler\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\"--fname\", type=str, required=True)\n",
        "  parser.add_argument(\"--devices\", type=str, default=\"cuda:0\")\n",
        "  cli = parser.parse_args()\n",
        "\n",
        "  with open(cli.fname, \"r\") as f:\n",
        "    args = yaml.safe_load(f)\n",
        "  args[\"meta\"][\"device\"] = cli.devices\n",
        "\n",
        "  main(args)"
      ],
      "metadata": {
        "id": "VYrxsMtCS8bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \"/content/msn/logistic_eval.py\"\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the Creative Commons Attribution–NonCommercial 4.0 International License\n",
        "#\n",
        "# Modifications: Adapted for use in 'Self-supervised learning applied to unlabelled histopathology imaging data –\n",
        "# a comparison between Masked Siamese Networks and Self-Distillation with No Labels', 2025.\n",
        "#\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "import pprint\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import cyanure as cyan\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "from src.data_manager import init_data\n",
        "import src.deit as deit\n",
        "\n",
        "logging.basicConfig()\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--lambd', type=float, default=0.00025, help='regularization')\n",
        "parser.add_argument('--penalty', type=str, default='l2', choices=['l2', 'elastic-net'])\n",
        "parser.add_argument('--mask', type=float, default=0.0, help='MSN masking used only when extracting TRAIN embs')\n",
        "parser.add_argument('--preload', action='store_true', help='reuse precomputed embeddings if present')\n",
        "parser.add_argument('--fname', type=str, required=True, help='short run tag to append to files')\n",
        "parser.add_argument('--model-name', type=str, required=True, help='backbone (e.g., deit_small)')\n",
        "parser.add_argument('--pretrained', type=str, required=True, help='folder containing pretrained checkpoint + where to save embs')\n",
        "parser.add_argument('--device', type=str, default='cuda:0')\n",
        "parser.add_argument('--normalize', type=bool, default=True, help='row-center + L2 if cyanure.preprocess is unavailable')\n",
        "parser.add_argument('--root-path', type=str, default='/datasets/')\n",
        "parser.add_argument('--image-folder', type=str, default='pathmnist', help='must be \"pathmnist\" for our data_manager')\n",
        "parser.add_argument('--subset-path', type=str, default=None, help='(unused)')\n",
        "\n",
        "# ADAPTED FOR USE\n",
        "# eval split + train label-budget\n",
        "parser.add_argument('--eval-split', type=str, default='test', choices=['val', 'test'])\n",
        "parser.add_argument('--train-subset-frac', type=float, default=None)\n",
        "parser.add_argument('--train-subset-size', type=int, default=None)\n",
        "parser.add_argument('--train-subset-per-class', type=int, default=None)\n",
        "parser.add_argument('--train-subset-seed', type=int, default=42)\n",
        "\n",
        "_GLOBAL_SEED = 0\n",
        "np.random.seed(_GLOBAL_SEED)\n",
        "torch.manual_seed(_GLOBAL_SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    device = torch.device(args.device)\n",
        "    if 'cuda' in args.device:\n",
        "        torch.cuda.set_device(device)\n",
        "\n",
        "    # ADAPTED FOR USE\n",
        "    # file tags so preload is safe across different subsets/splits\n",
        "    subset_tag = 'pathmnist-train'\n",
        "    if args.train_subset_per_class is not None:\n",
        "        subset_tag += f'-pc{args.train_subset_per_class}-seed{args.train_subset_seed}'\n",
        "    elif args.train_subset_size is not None:\n",
        "        subset_tag += f'-n{args.train_subset_size}-seed{args.train_subset_seed}'\n",
        "    elif args.train_subset_frac is not None:\n",
        "        subset_tag += f'-frac{args.train_subset_frac}-seed{args.train_subset_seed}'\n",
        "\n",
        "    # ADAPTED FOR USE\n",
        "    train_embs_path = os.path.join(args.pretrained, f'train-features-{subset_tag}-{args.fname}.pth.tar')\n",
        "    eval_embs_path  = os.path.join(args.pretrained, f'{args.eval_split}-features-{args.fname}.pth.tar')\n",
        "    logger.info(f\"Train embs file: {train_embs_path}\")\n",
        "    logger.info(f\"Eval  embs file: {eval_embs_path}\")\n",
        "\n",
        "    pretrained_ckpt = os.path.join(args.pretrained, args.fname)\n",
        "\n",
        "    # ADAPTED FOR USE\n",
        "    # data transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(size=256, interpolation=InterpolationMode.BICUBIC),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    # ADAPTED FOR USE\n",
        "    # data loaders via data_manager\n",
        "    # train: apply label budget via train_subset_*\n",
        "    train_loader, _ = init_data(\n",
        "        transform=transform,\n",
        "        batch_size=16,\n",
        "        pin_mem=True,\n",
        "        num_workers=6,\n",
        "        world_size=1,\n",
        "        rank=0,\n",
        "        root_path=args.root_path,\n",
        "        image_folder=args.image_folder,\n",
        "        training=True,\n",
        "        copy_data=False,\n",
        "        drop_last=False,\n",
        "        split='train',\n",
        "        train_subset_frac=args.train_subset_frac,\n",
        "        train_subset_size=args.train_subset_size,\n",
        "        train_subset_per_class=args.train_subset_per_class,\n",
        "        train_subset_seed=args.train_subset_seed,\n",
        "    )\n",
        "\n",
        "    # eval\n",
        "    eval_loader, _ = init_data(\n",
        "        transform=transform,\n",
        "        batch_size=16,\n",
        "        pin_mem=True,\n",
        "        num_workers=6,\n",
        "        world_size=1,\n",
        "        rank=0,\n",
        "        root_path=args.root_path,\n",
        "        image_folder=args.image_folder,\n",
        "        training=False,\n",
        "        copy_data=False,\n",
        "        drop_last=False,\n",
        "        split=args.eval_split,\n",
        "    )\n",
        "\n",
        "    # initialise the model\n",
        "    encoder = init_model(device=device, pretrained=pretrained_ckpt, model_name=args.model_name)\n",
        "    encoder.eval()\n",
        "\n",
        "    # if train embeddings already computed, load file, otherwise, compute embeddings and save\n",
        "    if args.preload and os.path.exists(train_embs_path):\n",
        "        checkpoint = torch.load(train_embs_path, map_location='cpu')\n",
        "        embs, labs = checkpoint['embs'], checkpoint['labs']\n",
        "        logger.info(f'loaded TRAIN embs of shape {embs.shape}')\n",
        "    else:\n",
        "        embs, labs = make_embeddings(blocks=1, device=device, mask_frac=args.mask,\n",
        "                                     data_loader=train_loader, encoder=encoder)\n",
        "        os.makedirs(os.path.dirname(train_embs_path), exist_ok=True)\n",
        "        torch.save({'embs': embs, 'labs': labs}, train_embs_path)\n",
        "        logger.info(f'saved TRAIN embs of shape {embs.shape}')\n",
        "\n",
        "    # ADAPTED FOR USE\n",
        "    # normalise rows (cyanure if available, else manual)\n",
        "    embs_np = embs.numpy().astype(np.float32)\n",
        "    labs_np = labs.numpy().astype(np.int64)\n",
        "    try:\n",
        "        cyan.preprocess(embs_np, normalize=args.normalize, columns=False, centering=True)\n",
        "    except AttributeError:\n",
        "        if args.normalize:\n",
        "            embs_np -= embs_np.mean(axis=1, keepdims=True)\n",
        "            norms = np.linalg.norm(embs_np, axis=1, keepdims=True) + 1e-12\n",
        "            embs_np /= norms\n",
        "\n",
        "    # ADAPTED FOR USE\n",
        "    # fit multinomial logistic regression\n",
        "    try:\n",
        "        classifier = cyan.MultiClassifier(loss='multiclass-logistic', penalty=args.penalty, fit_intercept=False)\n",
        "        classifier.fit(\n",
        "            embs_np, labs_np,\n",
        "            it0=10,\n",
        "            lambd=args.lambd/len(embs_np),\n",
        "            lambd2=args.lambd/len(embs_np),\n",
        "            nthreads=-1,\n",
        "            tol=1e-3,\n",
        "            solver='auto',\n",
        "            seed=0,\n",
        "            max_epochs=100\n",
        "        )\n",
        "        train_score = float(classifier.score(embs_np, labs_np))\n",
        "    except AttributeError:\n",
        "        from sklearn.linear_model import LogisticRegression\n",
        "        C = max(1e-6, float(len(embs_np)) / float(args.lambd))\n",
        "        if args.penalty == 'elastic-net':\n",
        "            clf = LogisticRegression(\n",
        "                penalty='elasticnet', l1_ratio=0.5, solver='saga',\n",
        "                multi_class='multinomial', fit_intercept=False,\n",
        "                max_iter=2000, tol=1e-3, C=C, n_jobs=-1\n",
        "            )\n",
        "        else:\n",
        "            clf = LogisticRegression(\n",
        "                penalty='l2', solver='saga',\n",
        "                multi_class='multinomial', fit_intercept=False,\n",
        "                max_iter=2000, tol=1e-3, C=C, n_jobs=-1\n",
        "            )\n",
        "        clf.fit(embs_np, labs_np)\n",
        "\n",
        "        class _SkAdapter:\n",
        "            def __init__(self, m): self.m = m\n",
        "            def score(self, X, y): return float(self.m.score(X, y))\n",
        "        classifier = _SkAdapter(clf)\n",
        "        train_score = classifier.score(embs_np, labs_np)\n",
        "\n",
        "    logger.info(f'TRAIN score: {train_score:.4f}')\n",
        "\n",
        "    # if test embeddings already computed, load file, otherwise, compute embeddings and save\n",
        "    if args.preload and os.path.exists(eval_embs_path):\n",
        "        checkpoint = torch.load(eval_embs_path, map_location='cpu')\n",
        "        eval_embs, eval_labs = checkpoint['embs'], checkpoint['labs']\n",
        "        logger.info(f'loaded {args.eval_split.upper()} embs of shape {eval_embs.shape}')\n",
        "    else:\n",
        "        eval_embs, eval_labs = make_embeddings(blocks=1, device=device, mask_frac=0.0,\n",
        "                                               data_loader=eval_loader, encoder=encoder)\n",
        "        torch.save({'embs': eval_embs, 'labs': eval_labs}, eval_embs_path)\n",
        "        logger.info(f'saved {args.eval_split.upper()} embs of shape {eval_embs.shape}')\n",
        "\n",
        "    # ADAPTED FOR USE\n",
        "    eval_embs_np = eval_embs.numpy().astype(np.float32)\n",
        "    eval_labs_np = eval_labs.numpy().astype(np.int64)\n",
        "    try:\n",
        "        cyan.preprocess(eval_embs_np, normalize=args.normalize, columns=False, centering=True)\n",
        "    except AttributeError:\n",
        "        if args.normalize:\n",
        "            eval_embs_np -= eval_embs_np.mean(axis=1, keepdims=True)\n",
        "            norms = np.linalg.norm(eval_embs_np, axis=1, keepdims=True) + 1e-12\n",
        "            eval_embs_np /= norms\n",
        "\n",
        "    eval_score = classifier.score(eval_embs_np, eval_labs_np)\n",
        "    logger.info(f'{args.eval_split.upper()} score: {eval_score:.4f}\\n')\n",
        "    return eval_score\n",
        "\n",
        "\n",
        "def make_embeddings(blocks, device, mask_frac, data_loader, encoder, epochs=1):\n",
        "    ipe = len(data_loader)\n",
        "    z_mem, l_mem = [], []\n",
        "    for _ in range(epochs):\n",
        "        for itr, (imgs, labels) in enumerate(data_loader):\n",
        "            imgs = imgs.to(device)\n",
        "            with torch.no_grad():\n",
        "                z = encoder.forward_blocks(imgs, blocks, mask_frac).cpu()\n",
        "            labels = labels.cpu()\n",
        "            z_mem.append(z)\n",
        "            l_mem.append(labels)\n",
        "            if itr % 50 == 0:\n",
        "                logger.info(f'[{itr}/{ipe}]')\n",
        "    z_mem = torch.cat(z_mem, 0)\n",
        "    l_mem = torch.cat(l_mem, 0)\n",
        "    logger.info(z_mem.shape); logger.info(l_mem.shape)\n",
        "    return z_mem, l_mem\n",
        "\n",
        "\n",
        "def load_pretrained(encoder, pretrained):\n",
        "    checkpoint = torch.load(pretrained, map_location='cpu')\n",
        "    pretrained_dict = {k.replace('module.', ''): v for k, v in checkpoint['target_encoder'].items()}\n",
        "    for k, v in encoder.state_dict().items():\n",
        "        if k not in pretrained_dict:\n",
        "            logger.info(f'key \"{k}\" could not be found in loaded state dict')\n",
        "        elif pretrained_dict[k].shape != v.shape:\n",
        "            logger.info(f'key \"{k}\" is of different shape in model and loaded state dict')\n",
        "            pretrained_dict[k] = v\n",
        "    msg = encoder.load_state_dict(pretrained_dict, strict=False)\n",
        "    logger.info(f'loaded pretrained model with msg: {msg}')\n",
        "    try:\n",
        "        logger.info(f'loaded pretrained encoder from epoch: {checkpoint[\"epoch\"]} path: {pretrained}')\n",
        "    except Exception:\n",
        "        pass\n",
        "    del checkpoint\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def init_model(device, pretrained, model_name):\n",
        "    encoder = deit.__dict__[model_name]()\n",
        "    encoder.fc = None\n",
        "    encoder.to(device)\n",
        "    encoder = load_pretrained(encoder=encoder, pretrained=pretrained)\n",
        "    return encoder\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = parser.parse_args()\n",
        "    pp.pprint(vars(args))\n",
        "    main(args)"
      ],
      "metadata": {
        "id": "5s0yyaDtS8qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \"/content/msn/main.py\"\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the Creative Commons Attribution–NonCommercial 4.0 International License\n",
        "#\n",
        "\n",
        "import argparse\n",
        "\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "import pprint\n",
        "import yaml\n",
        "\n",
        "from src.msn_train import main as msn\n",
        "\n",
        "from src.utils import init_distributed\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    '--fname', type=str,\n",
        "    help='name of config file to load',\n",
        "    default='configs.yaml')\n",
        "parser.add_argument(\n",
        "    '--devices', type=str, nargs='+', default=['cuda:0'],\n",
        "    help='which devices to use on local machine')\n",
        "\n",
        "\n",
        "def process_main(rank, fname, world_size, devices):\n",
        "    import os\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = str(devices[rank].split(':')[-1])\n",
        "\n",
        "    import logging\n",
        "    logging.basicConfig()\n",
        "    logger = logging.getLogger()\n",
        "    if rank == 0:\n",
        "        logger.setLevel(logging.INFO)\n",
        "    else:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "\n",
        "    logger.info(f'called-params {fname}')\n",
        "\n",
        "    # -- load script params\n",
        "    params = None\n",
        "    with open(fname, 'r') as y_file:\n",
        "        params = yaml.load(y_file, Loader=yaml.FullLoader)\n",
        "        logger.info('loaded params...')\n",
        "        pp = pprint.PrettyPrinter(indent=4)\n",
        "        pp.pprint(params)\n",
        "\n",
        "    dump = os.path.join(params['logging']['folder'], 'params-msn-train.yaml')\n",
        "    with open(dump, 'w') as f:\n",
        "        yaml.dump(params, f)\n",
        "\n",
        "    world_size, rank = init_distributed(rank_and_world_size=(rank, world_size))\n",
        "    logger.info(f'Running... (rank: {rank}/{world_size})')\n",
        "\n",
        "    return msn(params)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    num_gpus = len(args.devices)\n",
        "    mp.spawn(\n",
        "        process_main,\n",
        "        nprocs=num_gpus,\n",
        "        args=(args.fname, num_gpus, args.devices))"
      ],
      "metadata": {
        "id": "9HO6gpjCS8u9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}